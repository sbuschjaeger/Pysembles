<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>pysembles.Metrics API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS_CHTML" integrity="sha256-kZafAc6mZvK3W3v1pHOcUix30OHQN6pU/NO2oFkqZVw=" crossorigin></script>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>pysembles.Metrics</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import numpy as np
import pandas as pd
import torch
import scipy

import torch
from torch import nn
from torch.autograd import Variable
import torchvision
import torchvision.transforms as transforms

from sklearn.metrics import make_scorer, accuracy_score

from pysembles.Utils import TransformTensorDataset

&#39;&#39;&#39;
Some common metrics and helper functions. 
TODO: Add detailed documentation for each function
&#39;&#39;&#39;

def diversity(model, data_loader):
    if not hasattr(model, &#34;estimators_&#34;):
        return 0
    model.eval()
    
    diversities = []
    for batch in data_loader:
        data, target = batch[0], batch[1]
        data, target = data.cuda(), target.cuda()
        data, target = Variable(data), Variable(target)

        with torch.no_grad():
            f_bar, base_preds = model.forward_with_base(data)
        
        if isinstance(model.loss_function, nn.MSELoss): 
            n_classes = f_bar.shape[1]
            n_preds = f_bar.shape[0]

            eye_matrix = torch.eye(n_classes).repeat(n_preds, 1, 1).cuda()
            D = 2.0*eye_matrix
        elif isinstance(model.loss_function, nn.NLLLoss):
            n_classes = f_bar.shape[1]
            n_preds = f_bar.shape[0]
            D = torch.eye(n_classes).repeat(n_preds, 1, 1).cuda()
            target_one_hot = torch.nn.functional.one_hot(target, num_classes = n_classes).type(model.get_float_type())

            eps = 1e-7
            diag_vector = target_one_hot*(1.0/(f_bar**2+eps))
            D.diagonal(dim1=-2, dim2=-1).copy_(diag_vector)
        elif isinstance(model.loss_function, nn.CrossEntropyLoss):
            n_preds = f_bar.shape[0]
            n_classes = f_bar.shape[1]
            f_bar_softmax = nn.functional.softmax(f_bar,dim=1)
            D = -1.0*torch.bmm(f_bar_softmax.unsqueeze(2), f_bar_softmax.unsqueeze(1))
            diag_vector = f_bar_softmax*(1.0-f_bar_softmax)
            D.diagonal(dim1=-2, dim2=-1).copy_(diag_vector)
        else:
            D = torch.tensor(1.0)

        batch_diversities = []
        for pred in base_preds:
            diff = pred - f_bar 
            covar = torch.bmm(diff.unsqueeze(1), torch.bmm(D, diff.unsqueeze(2))).squeeze()
            div = 1.0/model.n_estimators * 0.5 * covar
            batch_diversities.append(div)

        diversities.append(torch.stack(batch_diversities, dim = 1))
    div = torch.cat(diversities,dim=0)
    return div.sum(dim=1).mean(dim=0).item()

def loss(model, data_loader):
    model.eval()
    
    losses = []
    for batch in data_loader:
        data, target = batch[0], batch[1]
        data, target = data.cuda(), target.cuda()
        data, target = Variable(data), Variable(target)

        with torch.no_grad():
            pred = model(data)
        
        losses.append(model.loss_function(pred, target).mean().item())
    
    return np.mean(losses)

def predict_proba(model, data_loader):
    preds = []
    for batch in data_loader:
        data, target = batch[0], batch[1]
        data, target = data.cuda(), target.cuda()
        data, target = Variable(data), Variable(target)
        with torch.no_grad():
            pred = model(data)
            preds.append(pred.detach().cpu().numpy())

    return np.concatenate(preds,axis=0)

def avg_loss(model, data_loader):
    if not hasattr(model, &#34;estimators_&#34;):
        return 0
    model.eval()
    losses = []
    for batch in data_loader:
        data, target = batch[0], batch[1]
        data, target = data.cuda(), target.cuda()
        data, target = Variable(data), Variable(target)

        with torch.no_grad():
            f_bar, base_preds = model.forward_with_base(data)
        
        ilosses = []
        for base in base_preds:
            ilosses.append(model.loss_function(base, target).mean().item())
            
        losses.append(np.mean(ilosses))

    return np.mean(losses)

def avg_accurcay(model, data_loader):
    if not hasattr(model, &#34;estimators_&#34;):
        return 0
    model.eval()
    
    accuracies = []
    for batch in data_loader:
        data, target = batch[0], batch[1]
        data, target = data.cuda(), target.cuda()
        data, target = Variable(data), Variable(target)

        with torch.no_grad():
            _, base_preds = model.forward_with_base(data)
        
        iaccuracies = []
        for base in base_preds:
            iaccuracies.append( 100.0*(base.argmax(1) == target).type(model.get_float_type()) )
            
        accuracies.append(torch.cat(iaccuracies,dim=0).mean().item())

    return np.mean(accuracies)

def accuracy(model, data_loader):
    model.eval()
    accuracies = []
    for batch in data_loader:
        data, target = batch[0], batch[1]
        data, target = data.cuda(), target.cuda()
        data, target = Variable(data), Variable(target)

        with torch.no_grad():
            pred = model(data)
            acc = (pred.argmax(1) == target).type(model.get_float_type()).mean().item()
            accuracies.append( acc )

    return 100.0*np.mean(accuracies)

# def diversity(model, x, y):
#     # This is basically a copy/paste from the GNCLClasifier regularizer, which can also be used for 
#     # other classifier. I tried to do it with numpy first and I think it should work but I did not 
#     # really understand numpy&#39;s bmm variant, so I opted for the safe route here. 
#     # Also, pytorch seems a little faster due to gpu support
#     if not hasattr(model, &#34;estimators_&#34;):
#         return 0
#     model.eval()
    
#     x_tensor = torch.tensor(x)
#     y_tensor = torch.tensor(y)
#     dataset = TransformTensorDataset(x_tensor, y_tensor, transform=None)
#     test_loader = torch.utils.data.DataLoader(dataset, batch_size = model.batch_size)
    
#     diversities = []
#     for batch in test_loader:
#         data, target = batch[0], batch[1]
#         data, target = data.cuda(), target.cuda()
#         data, target = Variable(data), Variable(target)

#         with torch.no_grad():
#             f_bar, base_preds = model.forward_with_base(data)
        
#         if isinstance(model.loss_function, nn.MSELoss): 
#             n_classes = f_bar.shape[1]
#             n_preds = f_bar.shape[0]

#             eye_matrix = torch.eye(n_classes).repeat(n_preds, 1, 1).cuda()
#             D = 2.0*eye_matrix
#         elif isinstance(model.loss_function, nn.NLLLoss):
#             n_classes = f_bar.shape[1]
#             n_preds = f_bar.shape[0]
#             D = torch.eye(n_classes).repeat(n_preds, 1, 1).cuda()
#             target_one_hot = torch.nn.functional.one_hot(target, num_classes = n_classes).type(model.get_float_type())

#             eps = 1e-7
#             diag_vector = target_one_hot*(1.0/(f_bar**2+eps))
#             D.diagonal(dim1=-2, dim2=-1).copy_(diag_vector)
#         elif isinstance(model.loss_function, nn.CrossEntropyLoss):
#             n_preds = f_bar.shape[0]
#             n_classes = f_bar.shape[1]
#             f_bar_softmax = nn.functional.softmax(f_bar,dim=1)
#             D = -1.0*torch.bmm(f_bar_softmax.unsqueeze(2), f_bar_softmax.unsqueeze(1))
#             diag_vector = f_bar_softmax*(1.0-f_bar_softmax)
#             D.diagonal(dim1=-2, dim2=-1).copy_(diag_vector)
#         else:
#             D = torch.tensor(1.0)

#         batch_diversities = []
#         for pred in base_preds:
#             diff = pred - f_bar 
#             covar = torch.bmm(diff.unsqueeze(1), torch.bmm(D, diff.unsqueeze(2))).squeeze()
#             div = 1.0/model.n_estimators * 0.5 * covar
#             batch_diversities.append(div)

#         diversities.append(torch.stack(batch_diversities, dim = 1))
#     div = torch.cat(diversities,dim=0)
#     return div.sum(dim=1).mean(dim=0).item()
    
#     # dsum = torch.sum(torch.cat(diversities,dim=0), dim = 0)
#     # return dsum
#     # base_preds = []
#     # for e in model.estimators_:
#     #     ypred = apply_in_batches(e, x, 128)
#     #     base_preds.append(ypred)
    
#     # f_bar = np.mean(base_preds, axis=0)
#     # if isinstance(model.loss_function, nn.MSELoss): 
#     #     n_classes = f_bar.shape[1]
#     #     n_preds = f_bar.shape[0]

#     #     eye_matrix = np.eye(n_classes).repeat(n_preds, 1, 1)
#     #     D = 2.0*eye_matrix
#     # elif isinstance(model.loss_function, nn.NLLLoss):
#     #     n_classes = f_bar.shape[1]
#     #     n_preds = f_bar.shape[0]
#     #     D = np.eye(n_classes).repeat(n_preds, 1, 1)
#     #     target_one_hot = np.zeros((y.size, n_classes))
#     #     target_one_hot[np.arange(y.size),y] = 1

#     #     eps = 1e-7
#     #     diag_vector = target_one_hot*(1.0/(f_bar**2+eps))
#     #     #D[np.diag_indices(D.shape[0])] = diag_vector
#     #     for i in range(D.shape[0]):
#     #         np.fill_diagonal(D[i,:], diag_vector[i,:])
#     # elif isinstance(model.loss_function, nn.CrossEntropyLoss):
#     #     n_preds = f_bar.shape[0]
#     #     n_classes = f_bar.shape[1]
#     #     f_bar_softmax = scipy.special.softmax(f_bar,axis=1)

#     #     D = -1.0 * np.expand_dims(f_bar_softmax, axis=2) @ np.expand_dims(f_bar_softmax, axis=1)

#     #     # D = -1.0*torch.bmm(f_bar_softmax.unsqueeze(2), f_bar_softmax.unsqueeze(1))
#     #     diag_vector = f_bar_softmax*(1.0-f_bar_softmax)
#     #     for i in range(D.shape[0]):
#     #         np.fill_diagonal(D[i,:], diag_vector[i,:])
#     # else:
#     #     D = np.array([1.0])

#     # diversities = []
#     # for pred in base_preds:
#     #     # https://stackoverflow.com/questions/63301019/dot-product-of-two-numpy-arrays-with-3d-vectors
#     #     # https://stackoverflow.com/questions/51479148/how-to-perform-a-stacked-element-wise-matrix-vector-multiplication-in-numpy
#     #     diff = pred - f_bar 
#     #     tmp = np.sum(D * diff[:,:,None], axis=1)
#     #     covar = np.sum(tmp*diff,axis=1)

#     #     # covar = torch.bmm(diff.unsqueeze(1), torch.bmm(D, diff.unsqueeze(2))).squeeze()
#     #     div = 1.0/model.n_estimators * 0.5 * covar
#     #     diversities.append(np.mean(div))
#     #return np.sum(diversities)

# def loss(model, x, y):
#     model.eval()
    
#     x_tensor = torch.tensor(x)
#     y_tensor = torch.tensor(y)
#     dataset = TransformTensorDataset(x_tensor, y_tensor, transform=None)
#     test_loader = torch.utils.data.DataLoader(dataset, batch_size = model.batch_size)
    
#     losses = []
#     for batch in test_loader:
#         data, target = batch[0], batch[1]
#         data, target = data.cuda(), target.cuda()
#         data, target = Variable(data), Variable(target)

#         with torch.no_grad():
#             pred = model(data)
        
#         losses.append(model.loss_function(pred, target).mean().item())
    
#     return np.mean(losses)

# def avg_loss(model, x, y):
#     if not hasattr(model, &#34;estimators_&#34;):
#         return 0
#     model.eval()
    
#     x_tensor = torch.tensor(x)
#     y_tensor = torch.tensor(y)
#     dataset = TransformTensorDataset(x_tensor, y_tensor, transform=None)
#     test_loader = torch.utils.data.DataLoader(dataset, batch_size = model.batch_size)
    
#     losses = []
#     for batch in test_loader:
#         data, target = batch[0], batch[1]
#         data, target = data.cuda(), target.cuda()
#         data, target = Variable(data), Variable(target)

#         with torch.no_grad():
#             f_bar, base_preds = model.forward_with_base(data)
        
#         ilosses = []
#         for base in base_preds:
#             ilosses.append(model.loss_function(base, target).mean().item())
            
#         losses.append(np.mean(ilosses))

#     return np.mean(losses)

# def avg_accurcay(model, x, y):
#     if not hasattr(model, &#34;estimators_&#34;):
#         return 0
#     model.eval()
    
#     x_tensor = torch.tensor(x)
#     y_tensor = torch.tensor(y)
#     dataset = TransformTensorDataset(x_tensor, y_tensor, transform=None)
#     test_loader = torch.utils.data.DataLoader(dataset, batch_size = model.batch_size)
    
#     accuracies = []
#     for batch in test_loader:
#         data, target = batch[0], batch[1]
#         data, target = data.cuda(), target.cuda()
#         data, target = Variable(data), Variable(target)

#         with torch.no_grad():
#             _, base_preds = model.forward_with_base(data)
        
#         iaccuracies = []
#         for base in base_preds:
#             iaccuracies.append( 100.0*(base.argmax(1) == target).type(model.get_float_type()) )
            
#         accuracies.append(torch.cat(iaccuracies,dim=0).mean().item())

#     return np.mean(accuracies)
#     # accuracies = torch.cat(accuracies,dim=0)
#     # return accuracies.mean().item()</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="pysembles.Metrics.accuracy"><code class="name flex">
<span>def <span class="ident">accuracy</span></span>(<span>model, data_loader)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def accuracy(model, data_loader):
    model.eval()
    accuracies = []
    for batch in data_loader:
        data, target = batch[0], batch[1]
        data, target = data.cuda(), target.cuda()
        data, target = Variable(data), Variable(target)

        with torch.no_grad():
            pred = model(data)
            acc = (pred.argmax(1) == target).type(model.get_float_type()).mean().item()
            accuracies.append( acc )

    return 100.0*np.mean(accuracies)</code></pre>
</details>
</dd>
<dt id="pysembles.Metrics.avg_accurcay"><code class="name flex">
<span>def <span class="ident">avg_accurcay</span></span>(<span>model, data_loader)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def avg_accurcay(model, data_loader):
    if not hasattr(model, &#34;estimators_&#34;):
        return 0
    model.eval()
    
    accuracies = []
    for batch in data_loader:
        data, target = batch[0], batch[1]
        data, target = data.cuda(), target.cuda()
        data, target = Variable(data), Variable(target)

        with torch.no_grad():
            _, base_preds = model.forward_with_base(data)
        
        iaccuracies = []
        for base in base_preds:
            iaccuracies.append( 100.0*(base.argmax(1) == target).type(model.get_float_type()) )
            
        accuracies.append(torch.cat(iaccuracies,dim=0).mean().item())

    return np.mean(accuracies)</code></pre>
</details>
</dd>
<dt id="pysembles.Metrics.avg_loss"><code class="name flex">
<span>def <span class="ident">avg_loss</span></span>(<span>model, data_loader)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def avg_loss(model, data_loader):
    if not hasattr(model, &#34;estimators_&#34;):
        return 0
    model.eval()
    losses = []
    for batch in data_loader:
        data, target = batch[0], batch[1]
        data, target = data.cuda(), target.cuda()
        data, target = Variable(data), Variable(target)

        with torch.no_grad():
            f_bar, base_preds = model.forward_with_base(data)
        
        ilosses = []
        for base in base_preds:
            ilosses.append(model.loss_function(base, target).mean().item())
            
        losses.append(np.mean(ilosses))

    return np.mean(losses)</code></pre>
</details>
</dd>
<dt id="pysembles.Metrics.diversity"><code class="name flex">
<span>def <span class="ident">diversity</span></span>(<span>model, data_loader)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def diversity(model, data_loader):
    if not hasattr(model, &#34;estimators_&#34;):
        return 0
    model.eval()
    
    diversities = []
    for batch in data_loader:
        data, target = batch[0], batch[1]
        data, target = data.cuda(), target.cuda()
        data, target = Variable(data), Variable(target)

        with torch.no_grad():
            f_bar, base_preds = model.forward_with_base(data)
        
        if isinstance(model.loss_function, nn.MSELoss): 
            n_classes = f_bar.shape[1]
            n_preds = f_bar.shape[0]

            eye_matrix = torch.eye(n_classes).repeat(n_preds, 1, 1).cuda()
            D = 2.0*eye_matrix
        elif isinstance(model.loss_function, nn.NLLLoss):
            n_classes = f_bar.shape[1]
            n_preds = f_bar.shape[0]
            D = torch.eye(n_classes).repeat(n_preds, 1, 1).cuda()
            target_one_hot = torch.nn.functional.one_hot(target, num_classes = n_classes).type(model.get_float_type())

            eps = 1e-7
            diag_vector = target_one_hot*(1.0/(f_bar**2+eps))
            D.diagonal(dim1=-2, dim2=-1).copy_(diag_vector)
        elif isinstance(model.loss_function, nn.CrossEntropyLoss):
            n_preds = f_bar.shape[0]
            n_classes = f_bar.shape[1]
            f_bar_softmax = nn.functional.softmax(f_bar,dim=1)
            D = -1.0*torch.bmm(f_bar_softmax.unsqueeze(2), f_bar_softmax.unsqueeze(1))
            diag_vector = f_bar_softmax*(1.0-f_bar_softmax)
            D.diagonal(dim1=-2, dim2=-1).copy_(diag_vector)
        else:
            D = torch.tensor(1.0)

        batch_diversities = []
        for pred in base_preds:
            diff = pred - f_bar 
            covar = torch.bmm(diff.unsqueeze(1), torch.bmm(D, diff.unsqueeze(2))).squeeze()
            div = 1.0/model.n_estimators * 0.5 * covar
            batch_diversities.append(div)

        diversities.append(torch.stack(batch_diversities, dim = 1))
    div = torch.cat(diversities,dim=0)
    return div.sum(dim=1).mean(dim=0).item()</code></pre>
</details>
</dd>
<dt id="pysembles.Metrics.loss"><code class="name flex">
<span>def <span class="ident">loss</span></span>(<span>model, data_loader)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def loss(model, data_loader):
    model.eval()
    
    losses = []
    for batch in data_loader:
        data, target = batch[0], batch[1]
        data, target = data.cuda(), target.cuda()
        data, target = Variable(data), Variable(target)

        with torch.no_grad():
            pred = model(data)
        
        losses.append(model.loss_function(pred, target).mean().item())
    
    return np.mean(losses)</code></pre>
</details>
</dd>
<dt id="pysembles.Metrics.predict_proba"><code class="name flex">
<span>def <span class="ident">predict_proba</span></span>(<span>model, data_loader)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def predict_proba(model, data_loader):
    preds = []
    for batch in data_loader:
        data, target = batch[0], batch[1]
        data, target = data.cuda(), target.cuda()
        data, target = Variable(data), Variable(target)
        with torch.no_grad():
            pred = model(data)
            preds.append(pred.detach().cpu().numpy())

    return np.concatenate(preds,axis=0)</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="pysembles" href="index.html">pysembles</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="two-column">
<li><code><a title="pysembles.Metrics.accuracy" href="#pysembles.Metrics.accuracy">accuracy</a></code></li>
<li><code><a title="pysembles.Metrics.avg_accurcay" href="#pysembles.Metrics.avg_accurcay">avg_accurcay</a></code></li>
<li><code><a title="pysembles.Metrics.avg_loss" href="#pysembles.Metrics.avg_loss">avg_loss</a></code></li>
<li><code><a title="pysembles.Metrics.diversity" href="#pysembles.Metrics.diversity">diversity</a></code></li>
<li><code><a title="pysembles.Metrics.loss" href="#pysembles.Metrics.loss">loss</a></code></li>
<li><code><a title="pysembles.Metrics.predict_proba" href="#pysembles.Metrics.predict_proba">predict_proba</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>