<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>pysembles.SMCLClassifier API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS_CHTML" integrity="sha256-kZafAc6mZvK3W3v1pHOcUix30OHQN6pU/NO2oFkqZVw=" crossorigin></script>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>pysembles.SMCLClassifier</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">#!/usr/bin/env python3
import warnings

import torch
from torch import nn

from .Models import Ensemble


class SMCLClassifier(Ensemble):
    &#34;&#34;&#34; Stochastic Multiple Choice Learning Classifier.

    As often argued, diversity might be important for ensembles to work well. Stochastic Multiple Choice Learning (SMCL)
    enforces diversity by training each expert model on a subset of the training data for which it already works
    pretty well. Due to the random initialization each ensemble member is likely to perform better or worse on different
    parts of the data and thereby introducing diversity. SMCL enforces this specialization by selecting the best
    expert (wrt. the loss) for each example and then only updates that one expert for that example. All other experts
    will never receive that example. 

    __References__

    [1] Lee, S., Purushwalkam, S., Cogswell, M., Ranjan, V., Crandall, D., &amp; Batra, D. (2016). Stochastic multiple choice learning for training diverse deep ensembles. Advances in Neural Information Processing Systems, 1(Nips), 2127–2135. Retrieved from http://papers.nips.cc/paper/6270-stochastic-multiple-choice-learning-for-training-diverse-deep-ensembles.pdf
    &#34;&#34;&#34;
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.estimators_ = nn.ModuleList([ self.base_estimator() for _ in range(self.n_estimators)])

        if self.combination_type is not None and self.combination_type != &#34;best&#34;:
            warnings.warn(&#34;SMCL is usually evaluated on the oracle loss, which means the _best_ predictor across the ensemble is used. You explicitly set it to &#39;{}&#39; which does not make sense. I am going to fix that for you now&#34;.format(self.combination_type)) 
        self.combination_type = &#34;best&#34;

    def prepare_backward(self, data, target, weights = None):
        f_bar, base_preds = self.forward_with_base(data)

        losses = []
        accuracies = []
        for i, pred in enumerate(base_preds):
            if weights is None:
                iloss = self.loss_function(pred, target)
            else:
                # TODO: PyTorch copies the weight vector if we use weights[:,i] to index
                #       a specific row. Maybe we should re-factor this?
                iloss = self.loss_function(pred, target) * weights[:,i].cuda()

            losses.append(iloss)
            accuracies.append(100.0*(pred.argmax(1) == target).type(self.get_float_type()))

        losses = torch.stack(losses, dim = 1)
        lmin, _ = losses.min(dim=1)
        accuracies = torch.stack(accuracies, dim = 1)

        d = {
            &#34;prediction&#34; : f_bar, 
            &#34;backward&#34; : lmin, 
            &#34;metrics&#34; :
            {
                &#34;loss&#34; : self.loss_function(f_bar, target),
                &#34;accuracy&#34; : 100.0*(f_bar.argmax(1) == target).type(self.get_float_type()), 
                &#34;avg loss&#34;: losses.mean(dim=1),
                &#34;avg accuracy&#34;: accuracies.mean(dim = 1),
            } 
            
        }
        return d</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="pysembles.SMCLClassifier.SMCLClassifier"><code class="flex name class">
<span>class <span class="ident">SMCLClassifier</span></span>
<span>(</span><span>*args, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Stochastic Multiple Choice Learning Classifier.</p>
<p>As often argued, diversity might be important for ensembles to work well. Stochastic Multiple Choice Learning (SMCL)
enforces diversity by training each expert model on a subset of the training data for which it already works
pretty well. Due to the random initialization each ensemble member is likely to perform better or worse on different
parts of the data and thereby introducing diversity. SMCL enforces this specialization by selecting the best
expert (wrt. the loss) for each example and then only updates that one expert for that example. All other experts
will never receive that example. </p>
<p><strong>References</strong></p>
<p>[1] Lee, S., Purushwalkam, S., Cogswell, M., Ranjan, V., Crandall, D., &amp; Batra, D. (2016). Stochastic multiple choice learning for training diverse deep ensembles. Advances in Neural Information Processing Systems, 1(Nips), 2127–2135. Retrieved from <a href="http://papers.nips.cc/paper/6270-stochastic-multiple-choice-learning-for-training-diverse-deep-ensembles.pdf">http://papers.nips.cc/paper/6270-stochastic-multiple-choice-learning-for-training-diverse-deep-ensembles.pdf</a></p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class SMCLClassifier(Ensemble):
    &#34;&#34;&#34; Stochastic Multiple Choice Learning Classifier.

    As often argued, diversity might be important for ensembles to work well. Stochastic Multiple Choice Learning (SMCL)
    enforces diversity by training each expert model on a subset of the training data for which it already works
    pretty well. Due to the random initialization each ensemble member is likely to perform better or worse on different
    parts of the data and thereby introducing diversity. SMCL enforces this specialization by selecting the best
    expert (wrt. the loss) for each example and then only updates that one expert for that example. All other experts
    will never receive that example. 

    __References__

    [1] Lee, S., Purushwalkam, S., Cogswell, M., Ranjan, V., Crandall, D., &amp; Batra, D. (2016). Stochastic multiple choice learning for training diverse deep ensembles. Advances in Neural Information Processing Systems, 1(Nips), 2127–2135. Retrieved from http://papers.nips.cc/paper/6270-stochastic-multiple-choice-learning-for-training-diverse-deep-ensembles.pdf
    &#34;&#34;&#34;
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.estimators_ = nn.ModuleList([ self.base_estimator() for _ in range(self.n_estimators)])

        if self.combination_type is not None and self.combination_type != &#34;best&#34;:
            warnings.warn(&#34;SMCL is usually evaluated on the oracle loss, which means the _best_ predictor across the ensemble is used. You explicitly set it to &#39;{}&#39; which does not make sense. I am going to fix that for you now&#34;.format(self.combination_type)) 
        self.combination_type = &#34;best&#34;

    def prepare_backward(self, data, target, weights = None):
        f_bar, base_preds = self.forward_with_base(data)

        losses = []
        accuracies = []
        for i, pred in enumerate(base_preds):
            if weights is None:
                iloss = self.loss_function(pred, target)
            else:
                # TODO: PyTorch copies the weight vector if we use weights[:,i] to index
                #       a specific row. Maybe we should re-factor this?
                iloss = self.loss_function(pred, target) * weights[:,i].cuda()

            losses.append(iloss)
            accuracies.append(100.0*(pred.argmax(1) == target).type(self.get_float_type()))

        losses = torch.stack(losses, dim = 1)
        lmin, _ = losses.min(dim=1)
        accuracies = torch.stack(accuracies, dim = 1)

        d = {
            &#34;prediction&#34; : f_bar, 
            &#34;backward&#34; : lmin, 
            &#34;metrics&#34; :
            {
                &#34;loss&#34; : self.loss_function(f_bar, target),
                &#34;accuracy&#34; : 100.0*(f_bar.argmax(1) == target).type(self.get_float_type()), 
                &#34;avg loss&#34;: losses.mean(dim=1),
                &#34;avg accuracy&#34;: accuracies.mean(dim = 1),
            } 
            
        }
        return d</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="pysembles.Models.Ensemble" href="Models.html#pysembles.Models.Ensemble">Ensemble</a></li>
<li><a title="pysembles.Models.BaseModel" href="Models.html#pysembles.Models.BaseModel">BaseModel</a></li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="pysembles.SMCLClassifier.SMCLClassifier.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="pysembles.SMCLClassifier.SMCLClassifier.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="pysembles.SMCLClassifier.SMCLClassifier.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, X) ‑> Callable[..., Any]</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, X):
    return self.forward_with_base(X)[0]</code></pre>
</details>
</dd>
<dt id="pysembles.SMCLClassifier.SMCLClassifier.prepare_backward"><code class="name flex">
<span>def <span class="ident">prepare_backward</span></span>(<span>self, data, target, weights=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def prepare_backward(self, data, target, weights = None):
    f_bar, base_preds = self.forward_with_base(data)

    losses = []
    accuracies = []
    for i, pred in enumerate(base_preds):
        if weights is None:
            iloss = self.loss_function(pred, target)
        else:
            # TODO: PyTorch copies the weight vector if we use weights[:,i] to index
            #       a specific row. Maybe we should re-factor this?
            iloss = self.loss_function(pred, target) * weights[:,i].cuda()

        losses.append(iloss)
        accuracies.append(100.0*(pred.argmax(1) == target).type(self.get_float_type()))

    losses = torch.stack(losses, dim = 1)
    lmin, _ = losses.min(dim=1)
    accuracies = torch.stack(accuracies, dim = 1)

    d = {
        &#34;prediction&#34; : f_bar, 
        &#34;backward&#34; : lmin, 
        &#34;metrics&#34; :
        {
            &#34;loss&#34; : self.loss_function(f_bar, target),
            &#34;accuracy&#34; : 100.0*(f_bar.argmax(1) == target).type(self.get_float_type()), 
            &#34;avg loss&#34;: losses.mean(dim=1),
            &#34;avg accuracy&#34;: accuracies.mean(dim = 1),
        } 
        
    }
    return d</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="pysembles" href="index.html">pysembles</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="pysembles.SMCLClassifier.SMCLClassifier" href="#pysembles.SMCLClassifier.SMCLClassifier">SMCLClassifier</a></code></h4>
<ul class="">
<li><code><a title="pysembles.SMCLClassifier.SMCLClassifier.dump_patches" href="#pysembles.SMCLClassifier.SMCLClassifier.dump_patches">dump_patches</a></code></li>
<li><code><a title="pysembles.SMCLClassifier.SMCLClassifier.forward" href="#pysembles.SMCLClassifier.SMCLClassifier.forward">forward</a></code></li>
<li><code><a title="pysembles.SMCLClassifier.SMCLClassifier.prepare_backward" href="#pysembles.SMCLClassifier.SMCLClassifier.prepare_backward">prepare_backward</a></code></li>
<li><code><a title="pysembles.SMCLClassifier.SMCLClassifier.training" href="#pysembles.SMCLClassifier.SMCLClassifier.training">training</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>