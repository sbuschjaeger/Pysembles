<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>pysembles.Utils API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS_CHTML" integrity="sha256-kZafAc6mZvK3W3v1pHOcUix30OHQN6pU/NO2oFkqZVw=" crossorigin></script>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>pysembles.Utils</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python"># from collections import OrderedDict
from functools import partial
import inspect
import warnings

import numpy as np
import copy 

import torch
from torch import nn
from torch.utils.data import Dataset
from torch.autograd import Variable

# from torch.autograd import Variable
# from torch.optim.optimizer import Optimizer, required

import torchvision
import torchvision.transforms as transforms

from pysembles.models.BinarisedNeuralNetworks import binarize, BinaryTanh, BinaryLinear, BinaryConv2d, Scale

# This function can be used as a scoring metric to score the number of parameters
def pytorch_total_params(model):
    return sum(p.numel() for p in model.parameters())

def flatten_dict(d):
    flat_dict = {}
    for k, v in d.items():
        if isinstance(v, dict):
            flat_d = flatten_dict(v)
            for k2, v2 in flat_d.items():
                flat_dict[k + &#34;_&#34; + k2] = v2
        else:
            flat_dict[k] = v
    return flat_dict

def replace_objects(d):
    d = d.copy()
    for k, v in d.items():
        if isinstance(v, dict):
            d[k] = replace_objects(v)
        elif isinstance(v, partial):
            d[k] = v.func.__name__ + &#34;_&#34; + &#34;_&#34;.join([str(arg) for arg in v.args])
        elif callable(v) or inspect.isclass(v):
            try:
                d[k] = v.__name__
            except Exception as e:
                d[k] = str(v) #.__name__
    return d

def dict_to_str(d):
    return str(replace_objects(d)).replace(&#34;:&#34;,&#34;=&#34;).replace(&#34;,&#34;,&#34;_&#34;).replace(&#34;\&#34;&#34;,&#34;&#34;).replace(&#34;\&#39;&#34;,&#34;&#34;).replace(&#34;{&#34;,&#34;&#34;).replace(&#34;}&#34;,&#34;&#34;).replace(&#34; &#34;, &#34;&#34;)

def replace_layer_if_possible(layer):
    class Sign(nn.Module):
        def __init__(self):
            super(Sign, self).__init__()

        def forward(self, input):
            return torch.where(input &gt; 0, torch.tensor([1.0]).cuda(), torch.tensor([-1.0]).cuda())
    
    if isinstance(layer, BinaryTanh):
        new_layer = Sign()
    elif isinstance(layer, BinaryLinear):
        new_layer = nn.Linear(layer.in_features, layer.out_features, hasattr(layer, &#39;bias&#39;))
        if hasattr(layer, &#39;bias&#39;):
            new_layer.bias.data = binarize(layer.bias).data
        new_layer.weight.data = binarize(layer.weight).data
    elif isinstance(layer, BinaryConv2d):
        new_layer = nn.Conv2d(
            layer.in_channels, layer.out_channels, layer.kernel_size, 
            layer.stride, layer.padding, layer.dilation, layer.groups, 
            hasattr(layer, &#39;bias&#39;), layer.padding_mode
        )
        if hasattr(layer, &#39;bias&#39;):
            new_layer.bias.data = binarize(layer.bias).data
        new_layer.weight.data = binarize(layer.weight).data
    else:
        new_layer = layer
    return new_layer

def replace_sequential_if_possible(s):
    for i,si in enumerate(s):
        if hasattr(s[i], &#34;layers_&#34;):
            s[i].layers_ = replace_sequential_if_possible(s[i].layers_)
        if isinstance(s[i], nn.Sequential):
            s[i] = replace_sequential_if_possible(s[i])
        else:
            s[i] = replace_layer_if_possible(s[i])
        # new_seq.append(tmp)
    return s

def store_model(model, path, dim, verbose = False):
    # Since we change layers in-place we copy it beforehand
    model = copy.deepcopy(model)
    print(&#34;BEFORE REPLACE:&#34;, model)

    model.layers_ = replace_sequential_if_possible(model.layers_)
    # for i in range(len(model.layers_)):
    #     model.layers_[i] = replace_layer_if_possible(model.layers_[i])

    #new_modules = OrderedDict()
    #for m_name, m in model._modules.items():
    # for m in model.modules():
    #     #new_modules[m_name] = replace_if_possible(m)
    #     if isinstance(m, nn.Sequential):
    #         m = replace_sequential_if_possible(m)
    #     else:
    #         m = replace_layer_if_possible(m)
    #     #m = replace_if_possible(m)
    
    print(&#34;AFTER REPLACE:&#34;, model)
    dummy_input = torch.randn((1,*dim), device=&#34;cuda&#34;)
    with warnings.catch_warnings():
        warnings.simplefilter(&#34;ignore&#34;)
        torch.onnx.export(model.cuda(), dummy_input, path, verbose=verbose, input_names=[&#34;input&#34;], output_names=[&#34;output&#34;])

# See: https://github.com/pytorch/pytorch/issues/19037
def cov(x, rowvar=False, bias=False, ddof=None, aweights=None):
    &#34;&#34;&#34;Estimates covariance matrix like numpy.cov&#34;&#34;&#34;
    # ensure at least 2D
    if x.dim() == 1:
        x = x.view(-1, 1)

    # treat each column as a data point, each row as a variable
    if rowvar and x.shape[0] != 1:
        x = x.t()

    if ddof is None:
        if bias == 0:
            ddof = 1
        else:
            ddof = 0

    w = aweights
    if w is not None:
        if not torch.is_tensor(w):
            w = torch.tensor(w, dtype=torch.float)
        w_sum = torch.sum(w)
        avg = torch.sum(x * (w/w_sum)[:,None], 0)
    else:
        avg = torch.mean(x, 0)

    # Determine the normalization
    if w is None:
        fact = x.shape[0] - ddof
    elif ddof == 0:
        fact = w_sum
    elif aweights is None:
        fact = w_sum - ddof
    else:
        fact = w_sum - ddof * torch.sum(w * w) / w_sum

    xm = x.sub(avg.expand_as(x))

    if w is None:
        X_T = xm.t()
    else:
        X_T = torch.mm(torch.diag(w), xm).t()

    c = torch.mm(X_T, xm)
    c = c / fact
    return c.squeeze()

def is_same_func(f1,f2):
    if isinstance(f1, partial) and isinstance(f2, partial):
        return f1.func == f2.func and f1.args == f2.args and f1.keywords == f2.keywords
    elif isinstance(f1, partial):
        return f1.func == f2
    elif isinstance(f1, partial):
        return f2.func == f1
    else:
        return f1 == f2

def apply_in_batches(fun, X, batch_size):
    x_tensor = torch.tensor(X)

    dataset = TransformTensorDataset(x_tensor, transform=None)
    test_loader = torch.utils.data.DataLoader(dataset, batch_size = batch_size)
    
    values = None
    for batch in test_loader:
        test_data = batch
        test_data = test_data.cuda()
        test_data = Variable(test_data)

        val = fun(test_data)
        val = val.cpu().detach().numpy()
        
        if values is None:
            values = val
        else:
            values = np.concatenate( (values, val), axis=0 )

    return values

# See: https://stackoverflow.com/questions/55588201/pytorch-transforms-on-tensordataset/55593757
class TransformTensorDataset(Dataset):
    &#34;&#34;&#34;TensorDataset with support of transforms.
    &#34;&#34;&#34;
    def __init__(self, x_tensor, y_tensor = None, w_tensor = None, transform=None):
        self.x = x_tensor
        self.y = y_tensor
        self.w = w_tensor
        self.transform = transform

    def __getitem__(self, index):
        x = self.x[index]

        if self.transform is not None:
            x = self.transform(x)
        
        if self.w is not None:
            y = self.y[index]
            w = self.w[index]
            return x, y, w
        elif self.y is not None:
            y = self.y[index]

            return x, y
        else:
            return x

    def __len__(self):
        return self.x.size(0)

class Clippy(torch.optim.Adam):
    def step(self, closure=None):
        loss = super(Clippy, self).step(closure=closure)
        for group in self.param_groups:
            for p in group[&#39;params&#39;]:
                p.data.clamp(-1,1)
            
        return loss

# SOME HELPFUL LAYERS
class Flatten(nn.Module):
    def __init__(self, store_shape=False):
        super(Flatten, self).__init__()
        self.store_shape = store_shape

    def forward(self, x):
        if self.store_shape:
            self.shape = x.shape

        return x.flatten(1)
        #return x.view(x.size(0), -1)

class Clamp(nn.Module):
    def __init__(self, min_out = -3, max_out = 3):
        super().__init__()
        self.min_out = min_out
        self.max_out = max_out

    def forward(self, input):
        return input.clamp(self.min_out, self.max_out)

class SkipConnection(nn.Module):
    def __init__(self, *block):
        super().__init__()
        self.layers_ = torch.nn.Sequential(*block)
    
    def forward(self, x):
        y = self.layers_(x)
        assert x.shape == y.shape
        return x + y</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="pysembles.Utils.apply_in_batches"><code class="name flex">
<span>def <span class="ident">apply_in_batches</span></span>(<span>fun, X, batch_size)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def apply_in_batches(fun, X, batch_size):
    x_tensor = torch.tensor(X)

    dataset = TransformTensorDataset(x_tensor, transform=None)
    test_loader = torch.utils.data.DataLoader(dataset, batch_size = batch_size)
    
    values = None
    for batch in test_loader:
        test_data = batch
        test_data = test_data.cuda()
        test_data = Variable(test_data)

        val = fun(test_data)
        val = val.cpu().detach().numpy()
        
        if values is None:
            values = val
        else:
            values = np.concatenate( (values, val), axis=0 )

    return values</code></pre>
</details>
</dd>
<dt id="pysembles.Utils.binarize"><code class="name flex">
<span>def <span class="ident">binarize</span></span>(<span>...)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="pysembles.Utils.cov"><code class="name flex">
<span>def <span class="ident">cov</span></span>(<span>x, rowvar=False, bias=False, ddof=None, aweights=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Estimates covariance matrix like numpy.cov</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def cov(x, rowvar=False, bias=False, ddof=None, aweights=None):
    &#34;&#34;&#34;Estimates covariance matrix like numpy.cov&#34;&#34;&#34;
    # ensure at least 2D
    if x.dim() == 1:
        x = x.view(-1, 1)

    # treat each column as a data point, each row as a variable
    if rowvar and x.shape[0] != 1:
        x = x.t()

    if ddof is None:
        if bias == 0:
            ddof = 1
        else:
            ddof = 0

    w = aweights
    if w is not None:
        if not torch.is_tensor(w):
            w = torch.tensor(w, dtype=torch.float)
        w_sum = torch.sum(w)
        avg = torch.sum(x * (w/w_sum)[:,None], 0)
    else:
        avg = torch.mean(x, 0)

    # Determine the normalization
    if w is None:
        fact = x.shape[0] - ddof
    elif ddof == 0:
        fact = w_sum
    elif aweights is None:
        fact = w_sum - ddof
    else:
        fact = w_sum - ddof * torch.sum(w * w) / w_sum

    xm = x.sub(avg.expand_as(x))

    if w is None:
        X_T = xm.t()
    else:
        X_T = torch.mm(torch.diag(w), xm).t()

    c = torch.mm(X_T, xm)
    c = c / fact
    return c.squeeze()</code></pre>
</details>
</dd>
<dt id="pysembles.Utils.dict_to_str"><code class="name flex">
<span>def <span class="ident">dict_to_str</span></span>(<span>d)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def dict_to_str(d):
    return str(replace_objects(d)).replace(&#34;:&#34;,&#34;=&#34;).replace(&#34;,&#34;,&#34;_&#34;).replace(&#34;\&#34;&#34;,&#34;&#34;).replace(&#34;\&#39;&#34;,&#34;&#34;).replace(&#34;{&#34;,&#34;&#34;).replace(&#34;}&#34;,&#34;&#34;).replace(&#34; &#34;, &#34;&#34;)</code></pre>
</details>
</dd>
<dt id="pysembles.Utils.flatten_dict"><code class="name flex">
<span>def <span class="ident">flatten_dict</span></span>(<span>d)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def flatten_dict(d):
    flat_dict = {}
    for k, v in d.items():
        if isinstance(v, dict):
            flat_d = flatten_dict(v)
            for k2, v2 in flat_d.items():
                flat_dict[k + &#34;_&#34; + k2] = v2
        else:
            flat_dict[k] = v
    return flat_dict</code></pre>
</details>
</dd>
<dt id="pysembles.Utils.is_same_func"><code class="name flex">
<span>def <span class="ident">is_same_func</span></span>(<span>f1, f2)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def is_same_func(f1,f2):
    if isinstance(f1, partial) and isinstance(f2, partial):
        return f1.func == f2.func and f1.args == f2.args and f1.keywords == f2.keywords
    elif isinstance(f1, partial):
        return f1.func == f2
    elif isinstance(f1, partial):
        return f2.func == f1
    else:
        return f1 == f2</code></pre>
</details>
</dd>
<dt id="pysembles.Utils.pytorch_total_params"><code class="name flex">
<span>def <span class="ident">pytorch_total_params</span></span>(<span>model)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def pytorch_total_params(model):
    return sum(p.numel() for p in model.parameters())</code></pre>
</details>
</dd>
<dt id="pysembles.Utils.replace_layer_if_possible"><code class="name flex">
<span>def <span class="ident">replace_layer_if_possible</span></span>(<span>layer)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def replace_layer_if_possible(layer):
    class Sign(nn.Module):
        def __init__(self):
            super(Sign, self).__init__()

        def forward(self, input):
            return torch.where(input &gt; 0, torch.tensor([1.0]).cuda(), torch.tensor([-1.0]).cuda())
    
    if isinstance(layer, BinaryTanh):
        new_layer = Sign()
    elif isinstance(layer, BinaryLinear):
        new_layer = nn.Linear(layer.in_features, layer.out_features, hasattr(layer, &#39;bias&#39;))
        if hasattr(layer, &#39;bias&#39;):
            new_layer.bias.data = binarize(layer.bias).data
        new_layer.weight.data = binarize(layer.weight).data
    elif isinstance(layer, BinaryConv2d):
        new_layer = nn.Conv2d(
            layer.in_channels, layer.out_channels, layer.kernel_size, 
            layer.stride, layer.padding, layer.dilation, layer.groups, 
            hasattr(layer, &#39;bias&#39;), layer.padding_mode
        )
        if hasattr(layer, &#39;bias&#39;):
            new_layer.bias.data = binarize(layer.bias).data
        new_layer.weight.data = binarize(layer.weight).data
    else:
        new_layer = layer
    return new_layer</code></pre>
</details>
</dd>
<dt id="pysembles.Utils.replace_objects"><code class="name flex">
<span>def <span class="ident">replace_objects</span></span>(<span>d)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def replace_objects(d):
    d = d.copy()
    for k, v in d.items():
        if isinstance(v, dict):
            d[k] = replace_objects(v)
        elif isinstance(v, partial):
            d[k] = v.func.__name__ + &#34;_&#34; + &#34;_&#34;.join([str(arg) for arg in v.args])
        elif callable(v) or inspect.isclass(v):
            try:
                d[k] = v.__name__
            except Exception as e:
                d[k] = str(v) #.__name__
    return d</code></pre>
</details>
</dd>
<dt id="pysembles.Utils.replace_sequential_if_possible"><code class="name flex">
<span>def <span class="ident">replace_sequential_if_possible</span></span>(<span>s)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def replace_sequential_if_possible(s):
    for i,si in enumerate(s):
        if hasattr(s[i], &#34;layers_&#34;):
            s[i].layers_ = replace_sequential_if_possible(s[i].layers_)
        if isinstance(s[i], nn.Sequential):
            s[i] = replace_sequential_if_possible(s[i])
        else:
            s[i] = replace_layer_if_possible(s[i])
        # new_seq.append(tmp)
    return s</code></pre>
</details>
</dd>
<dt id="pysembles.Utils.store_model"><code class="name flex">
<span>def <span class="ident">store_model</span></span>(<span>model, path, dim, verbose=False)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def store_model(model, path, dim, verbose = False):
    # Since we change layers in-place we copy it beforehand
    model = copy.deepcopy(model)
    print(&#34;BEFORE REPLACE:&#34;, model)

    model.layers_ = replace_sequential_if_possible(model.layers_)
    # for i in range(len(model.layers_)):
    #     model.layers_[i] = replace_layer_if_possible(model.layers_[i])

    #new_modules = OrderedDict()
    #for m_name, m in model._modules.items():
    # for m in model.modules():
    #     #new_modules[m_name] = replace_if_possible(m)
    #     if isinstance(m, nn.Sequential):
    #         m = replace_sequential_if_possible(m)
    #     else:
    #         m = replace_layer_if_possible(m)
    #     #m = replace_if_possible(m)
    
    print(&#34;AFTER REPLACE:&#34;, model)
    dummy_input = torch.randn((1,*dim), device=&#34;cuda&#34;)
    with warnings.catch_warnings():
        warnings.simplefilter(&#34;ignore&#34;)
        torch.onnx.export(model.cuda(), dummy_input, path, verbose=verbose, input_names=[&#34;input&#34;], output_names=[&#34;output&#34;])</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="pysembles.Utils.Clamp"><code class="flex name class">
<span>class <span class="ident">Clamp</span></span>
<span>(</span><span>min_out=-3, max_out=3)</span>
</code></dt>
<dd>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Clamp(nn.Module):
    def __init__(self, min_out = -3, max_out = 3):
        super().__init__()
        self.min_out = min_out
        self.max_out = max_out

    def forward(self, input):
        return input.clamp(self.min_out, self.max_out)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="pysembles.Utils.Clamp.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="pysembles.Utils.Clamp.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="pysembles.Utils.Clamp.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, input) ‑> Callable[..., Any]</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, input):
    return input.clamp(self.min_out, self.max_out)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="pysembles.Utils.Clippy"><code class="flex name class">
<span>class <span class="ident">Clippy</span></span>
<span>(</span><span>params, lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Implements Adam algorithm.</p>
<p>It has been proposed in <code>Adam: A Method for Stochastic Optimization</code>_.</p>
<h2 id="arguments">Arguments</h2>
<p>params (iterable): iterable of parameters to optimize or dicts defining
parameter groups
lr (float, optional): learning rate (default: 1e-3)
betas (Tuple[float, float], optional): coefficients used for computing
running averages of gradient and its square (default: (0.9, 0.999))
eps (float, optional): term added to the denominator to improve
numerical stability (default: 1e-8)
weight_decay (float, optional): weight decay (L2 penalty) (default: 0)
amsgrad (boolean, optional): whether to use the AMSGrad variant of this
algorithm from the paper <code>On the Convergence of Adam and Beyond</code>_
(default: False)</p>
<p>.. _Adam\: A Method for Stochastic Optimization:
<a href="https://arxiv.org/abs/1412.6980">https://arxiv.org/abs/1412.6980</a>
.. _On the Convergence of Adam and Beyond:
<a href="https://openreview.net/forum?id=ryQu7f-RZ">https://openreview.net/forum?id=ryQu7f-RZ</a></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Clippy(torch.optim.Adam):
    def step(self, closure=None):
        loss = super(Clippy, self).step(closure=closure)
        for group in self.param_groups:
            for p in group[&#39;params&#39;]:
                p.data.clamp(-1,1)
            
        return loss</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.optim.adam.Adam</li>
<li>torch.optim.optimizer.Optimizer</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="pysembles.Utils.Clippy.step"><code class="name flex">
<span>def <span class="ident">step</span></span>(<span>self, closure=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Performs a single optimization step.</p>
<h2 id="arguments">Arguments</h2>
<p>closure (callable, optional): A closure that reevaluates the model
and returns the loss.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def step(self, closure=None):
    loss = super(Clippy, self).step(closure=closure)
    for group in self.param_groups:
        for p in group[&#39;params&#39;]:
            p.data.clamp(-1,1)
        
    return loss</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="pysembles.Utils.Flatten"><code class="flex name class">
<span>class <span class="ident">Flatten</span></span>
<span>(</span><span>store_shape=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Flatten(nn.Module):
    def __init__(self, store_shape=False):
        super(Flatten, self).__init__()
        self.store_shape = store_shape

    def forward(self, x):
        if self.store_shape:
            self.shape = x.shape

        return x.flatten(1)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="pysembles.Utils.Flatten.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="pysembles.Utils.Flatten.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="pysembles.Utils.Flatten.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x) ‑> Callable[..., Any]</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, x):
    if self.store_shape:
        self.shape = x.shape

    return x.flatten(1)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="pysembles.Utils.SkipConnection"><code class="flex name class">
<span>class <span class="ident">SkipConnection</span></span>
<span>(</span><span>*block)</span>
</code></dt>
<dd>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class SkipConnection(nn.Module):
    def __init__(self, *block):
        super().__init__()
        self.layers_ = torch.nn.Sequential(*block)
    
    def forward(self, x):
        y = self.layers_(x)
        assert x.shape == y.shape
        return x + y</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="pysembles.Utils.SkipConnection.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="pysembles.Utils.SkipConnection.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="pysembles.Utils.SkipConnection.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x) ‑> Callable[..., Any]</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, x):
    y = self.layers_(x)
    assert x.shape == y.shape
    return x + y</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="pysembles.Utils.TransformTensorDataset"><code class="flex name class">
<span>class <span class="ident">TransformTensorDataset</span></span>
<span>(</span><span>x_tensor, y_tensor=None, w_tensor=None, transform=None)</span>
</code></dt>
<dd>
<div class="desc"><p>TensorDataset with support of transforms.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class TransformTensorDataset(Dataset):
    &#34;&#34;&#34;TensorDataset with support of transforms.
    &#34;&#34;&#34;
    def __init__(self, x_tensor, y_tensor = None, w_tensor = None, transform=None):
        self.x = x_tensor
        self.y = y_tensor
        self.w = w_tensor
        self.transform = transform

    def __getitem__(self, index):
        x = self.x[index]

        if self.transform is not None:
            x = self.transform(x)
        
        if self.w is not None:
            y = self.y[index]
            w = self.w[index]
            return x, y, w
        elif self.y is not None:
            y = self.y[index]

            return x, y
        else:
            return x

    def __len__(self):
        return self.x.size(0)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.utils.data.dataset.Dataset</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="pysembles" href="index.html">pysembles</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="pysembles.Utils.apply_in_batches" href="#pysembles.Utils.apply_in_batches">apply_in_batches</a></code></li>
<li><code><a title="pysembles.Utils.binarize" href="#pysembles.Utils.binarize">binarize</a></code></li>
<li><code><a title="pysembles.Utils.cov" href="#pysembles.Utils.cov">cov</a></code></li>
<li><code><a title="pysembles.Utils.dict_to_str" href="#pysembles.Utils.dict_to_str">dict_to_str</a></code></li>
<li><code><a title="pysembles.Utils.flatten_dict" href="#pysembles.Utils.flatten_dict">flatten_dict</a></code></li>
<li><code><a title="pysembles.Utils.is_same_func" href="#pysembles.Utils.is_same_func">is_same_func</a></code></li>
<li><code><a title="pysembles.Utils.pytorch_total_params" href="#pysembles.Utils.pytorch_total_params">pytorch_total_params</a></code></li>
<li><code><a title="pysembles.Utils.replace_layer_if_possible" href="#pysembles.Utils.replace_layer_if_possible">replace_layer_if_possible</a></code></li>
<li><code><a title="pysembles.Utils.replace_objects" href="#pysembles.Utils.replace_objects">replace_objects</a></code></li>
<li><code><a title="pysembles.Utils.replace_sequential_if_possible" href="#pysembles.Utils.replace_sequential_if_possible">replace_sequential_if_possible</a></code></li>
<li><code><a title="pysembles.Utils.store_model" href="#pysembles.Utils.store_model">store_model</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="pysembles.Utils.Clamp" href="#pysembles.Utils.Clamp">Clamp</a></code></h4>
<ul class="">
<li><code><a title="pysembles.Utils.Clamp.dump_patches" href="#pysembles.Utils.Clamp.dump_patches">dump_patches</a></code></li>
<li><code><a title="pysembles.Utils.Clamp.forward" href="#pysembles.Utils.Clamp.forward">forward</a></code></li>
<li><code><a title="pysembles.Utils.Clamp.training" href="#pysembles.Utils.Clamp.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="pysembles.Utils.Clippy" href="#pysembles.Utils.Clippy">Clippy</a></code></h4>
<ul class="">
<li><code><a title="pysembles.Utils.Clippy.step" href="#pysembles.Utils.Clippy.step">step</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="pysembles.Utils.Flatten" href="#pysembles.Utils.Flatten">Flatten</a></code></h4>
<ul class="">
<li><code><a title="pysembles.Utils.Flatten.dump_patches" href="#pysembles.Utils.Flatten.dump_patches">dump_patches</a></code></li>
<li><code><a title="pysembles.Utils.Flatten.forward" href="#pysembles.Utils.Flatten.forward">forward</a></code></li>
<li><code><a title="pysembles.Utils.Flatten.training" href="#pysembles.Utils.Flatten.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="pysembles.Utils.SkipConnection" href="#pysembles.Utils.SkipConnection">SkipConnection</a></code></h4>
<ul class="">
<li><code><a title="pysembles.Utils.SkipConnection.dump_patches" href="#pysembles.Utils.SkipConnection.dump_patches">dump_patches</a></code></li>
<li><code><a title="pysembles.Utils.SkipConnection.forward" href="#pysembles.Utils.SkipConnection.forward">forward</a></code></li>
<li><code><a title="pysembles.Utils.SkipConnection.training" href="#pysembles.Utils.SkipConnection.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="pysembles.Utils.TransformTensorDataset" href="#pysembles.Utils.TransformTensorDataset">TransformTensorDataset</a></code></h4>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>