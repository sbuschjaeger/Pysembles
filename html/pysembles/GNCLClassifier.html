<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>pysembles.GNCLClassifier API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS_CHTML" integrity="sha256-kZafAc6mZvK3W3v1pHOcUix30OHQN6pU/NO2oFkqZVw=" crossorigin></script>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>pysembles.GNCLClassifier</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">#!/usr/bin/env python3

import warnings
import torch
import torch.optim as optim
from sklearn.base import clone
from torch import nn
from tqdm import tqdm

from .Models import Ensemble
from .Utils import TransformTensorDataset

class GNCLClassifier(Ensemble):
    &#34;&#34;&#34;(Generalized) Negative Correlation Learning.

    Negative Correlation Learning uses the Bias-Variance-Co-Variance decomposition to derive a regularized objective function which enforces diversity among the ensemble members. The first appearance of this work was for the MSE loss in [1,2]. A modern application to the problem of crowd counting can be found in [4]. Optiz later proposed a similar objective using the Cross Entropy Loss, but without theoretical justifications [3]. Webb et al. in [5,6] gave more theoretical background for using the Cross Entropy Loss. Note that [6] is basically a shorter version of [5].

    We generalized and unified the previous works to include _any_ loss function using a second order Taylor approximation. As detailed in our paper, there is an upper bound for the GNCL objective which does not compute the exact diversity term. Thus, any differentiable loss \( \ell \) function can be used for optimization. We will refer to this mode as `upper`. Let \( f(x) \) be the ensembles prediction consisting of M models \( \{h^1, \dots, h^M\} \), then 

    $$
        \\frac{1}{N}\sum_{j=1}^N \lambda \ell(f(x_j),y_j) + \\frac{1-\lambda}{M}\sum_{i=1}^M \ell(h^i(x_j),y_j)
    $$

    where \( \lambda \in [0,1] \) is the trade-off between the ensemble&#39;s performance and its diversity.

    If you are interested in explicitly computing the diversity we currently supports three loss functions: MSE, Negative Log-Likelihood and CrossEntropy. We call this mode `exact`. In this case the following objective is used:
    $$
        \\frac{1}{M} \sum_{i=1}^M \ell(h^i) - \\frac{\lambda}{2M} \sum_{i=1}^M {d_i}^T D d_i
    $$
    where \( D = \\nabla^2_{f(x)}\ell(f(x), y), d_i = (h^i(x) - f(x))\) is optimized.

    Note that PyTorchs autograd functionality does not directly support a hessian (or even gradients) on a &#34;per example&#34; basis, but only summed over a batch. So far I have not found a way to use autograd efficiently here. Therefore, this mode is currently restricted to the three mentioned loss functions.
                
    Attributes:
        n_estimators (int): Number of estimators in ensemble. Should be at least 1
        
        l_reg (float): Trade-off between diversity and loss. Should be between 0 and 1. l_reg = 0 implies independent training whereas l_reg = 1 implies no independent training

        mode (str): The mode used for fitting a new model. 
            
            - `mode != &#34;exact&#34;`: The upper bound is used for fitting. 
            - `mode == &#34;exact&#34;`: The diversity is explicity computed and used for fitting. Only `nn.MSELoss`, `nn.NLLLoss`, `nn.CrossEntropyLoss` are supported.

    __References__

    [1] Liu, Y., &amp; Yao, X. (1999). Ensemble learning via negative correlation. Neural Networks, 12(10), 1399–1404. https://doi.org/10.1016/S0893-6080(99)00073-8 

    [2] Brown, G., WatT, J. L., &amp; Tino, P. (2005). Managing Diversity in Regression Ensembles. Jmlr, (6), 1621–1650. https://doi.org/10.1097/IYC.0000000000000008

    [3] Opitz, M., Possegger, H., &amp; Bischof, H. (2016). Efficient model averaging for deep neural networks. Asian Conference on Computer Vision, 10112 LNCS, 205–220. https://doi.org/10.1007/978-3-319-54184-6_13

    [4] Shi, Z., Zhang, L., Liu, Y., Cao, X., Ye, Y., Cheng, M., &amp; Zheng, G. (n.d.). Crowd Counting with Deep Negative Correlation Learning, 5382–5390. Retrieved from http://openaccess.thecvf.com/content_cvpr_2018/papers/Shi_Crowd_Counting_With_CVPR_2018_paper.pdf

    [5] Webb, A. M., Reynolds, C., Iliescu, D.-A., Reeve, H., Lujan, M., &amp; Brown, G. (2019). Joint Training of Neural Network Ensembles, (4), 1–14. https://doi.org/10.13140/RG.2.2.28091.46880

    [6] Webb, A. M., Reynolds, C., Chen, W., Reeve, H., Iliescu, D.-A., Lujan, M., &amp; Brown, G. (2020). To Ensemble or Not Ensemble: When does End-To-End Training Fail? In ECML PKDD 2020 (pp. 1–16). Retrieved from http://www.cs.man.ac.uk/~gbrown/publications/ecml2020webb.pdf
&#34;&#34;&#34;

    def __init__(self, l_reg = 0, mode = &#34;exact&#34;, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.l_reg = l_reg
        self.mode = mode
        self.estimators_ = nn.ModuleList([ self.base_estimator() for _ in range(self.n_estimators)])

        if self.mode == &#34;exact&#34; and not isinstance(self.loss_function, (nn.MSELoss, nn.NLLLoss, nn.CrossEntropyLoss)):
            warnings.warn(&#34;You set GNCL to &#39;exact&#39; but used an unsupported loss function for exact minimization. Currrently supported are MSELoss, NLLLoss, and CrossEntropyLoss. I am setting mode to &#39;upper&#39; now and minimize the upper bound using the provided loss function&#34;) 
            self.mode = &#34;upper&#34;

    def restore_state(self, checkpoint):
        super().restore_state(checkpoint)
        self.mode = checkpoint[&#34;mode&#34;]

    def get_state(self):
        state = super().get_state()
        return {
            **state,
            &#34;mode&#34;:self.mode,
        } 

    def prepare_backward(self, data, target, weights = None):
        # TODO Make use of the weights as well!
        f_bar, base_preds = self.forward_with_base(data)
        
        if self.mode == &#34;upper&#34;:
            n_classes = f_bar.shape[1]
            n_preds = f_bar.shape[0]
            D = torch.eye(n_classes).repeat(n_preds, 1, 1).cuda()
        else:
            if isinstance(self.loss_function, nn.MSELoss): 
                n_classes = f_bar.shape[1]
                n_preds = f_bar.shape[0]

                eye_matrix = torch.eye(n_classes).repeat(n_preds, 1, 1).cuda()
                D = 2.0*eye_matrix
            elif isinstance(self.loss_function, nn.NLLLoss):
                n_classes = f_bar.shape[1]
                n_preds = f_bar.shape[0]
                D = torch.eye(n_classes).repeat(n_preds, 1, 1).cuda()
                target_one_hot = torch.nn.functional.one_hot(target, num_classes = n_classes).type(self.get_float_type())

                eps = 1e-7
                diag_vector = target_one_hot*(1.0/(f_bar**2+eps))
                D.diagonal(dim1=-2, dim2=-1).copy_(diag_vector)
            elif isinstance(self.loss_function, nn.CrossEntropyLoss):
                n_preds = f_bar.shape[0]
                n_classes = f_bar.shape[1]
                f_bar_softmax = nn.functional.softmax(f_bar,dim=1)
                D = -1.0*torch.bmm(f_bar_softmax.unsqueeze(2), f_bar_softmax.unsqueeze(1))
                diag_vector = f_bar_softmax*(1.0-f_bar_softmax)
                D.diagonal(dim1=-2, dim2=-1).copy_(diag_vector)
            else:
                # NOTE: We should never reach this code path
                raise ValueError(&#34;Invalid combination of mode and loss function in GNCLClassifier.&#34;)

        losses = []
        accuracies = []
        diversity = []
        f_loss = self.loss_function(f_bar, target)
        for pred in base_preds:
            diff = pred - f_bar 
            covar = torch.bmm(diff.unsqueeze(1), torch.bmm(D, diff.unsqueeze(2))).squeeze()
            div = 1.0/self.n_estimators * 1.0/2.0 * covar
            i_loss = self.loss_function(pred, target)

            if self.mode == &#34;exact&#34;:
                # Eq. (4)
                reg_loss = 1.0/self.n_estimators * i_loss - self.l_reg * div
            else:
                # Eq. (5) where we scale the ensemble loss with 1.0/self.n_estimators due to the summation in line 118
                reg_loss = 1.0/self.n_estimators * self.l_reg * f_loss + (1.0 - self.l_reg)/self.n_estimators * i_loss
            
            losses.append(reg_loss)
            accuracies.append(100.0*(pred.argmax(1) == target).type(self.get_float_type()))
            diversity.append(div)

        losses = torch.stack(losses, dim = 1)
        accuracies = torch.stack(accuracies, dim = 1)
        diversity = torch.stack(diversity, dim = 1)
        
        # NOTE: avg loss is the average (regularized) loss and not the average loss (wrt. to the loss_function)
        d = {
            &#34;prediction&#34; : f_bar, 
            &#34;backward&#34; : losses.sum(dim=1), 
            &#34;metrics&#34; :
            {
                &#34;loss&#34; : f_loss,
                &#34;accuracy&#34; : 100.0*(f_bar.argmax(1) == target).type(self.get_float_type()), 
                &#34;avg loss&#34;: losses.mean(dim=1),
                &#34;avg accuracy&#34;: accuracies.mean(dim = 1),
                &#34;diversity&#34;: diversity.sum(dim = 1)
            } 
            
        }
        return d</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="pysembles.GNCLClassifier.GNCLClassifier"><code class="flex name class">
<span>class <span class="ident">GNCLClassifier</span></span>
<span>(</span><span>l_reg=0, mode='exact', *args, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>(Generalized) Negative Correlation Learning.</p>
<p>Negative Correlation Learning uses the Bias-Variance-Co-Variance decomposition to derive a regularized objective function which enforces diversity among the ensemble members. The first appearance of this work was for the MSE loss in [1,2]. A modern application to the problem of crowd counting can be found in [4]. Optiz later proposed a similar objective using the Cross Entropy Loss, but without theoretical justifications [3]. Webb et al. in [5,6] gave more theoretical background for using the Cross Entropy Loss. Note that [6] is basically a shorter version of [5].</p>
<p>We generalized and unified the previous works to include <em>any</em> loss function using a second order Taylor approximation. As detailed in our paper, there is an upper bound for the GNCL objective which does not compute the exact diversity term. Thus, any differentiable loss <span><span class="MathJax_Preview"> \ell </span><script type="math/tex"> \ell </script></span> function can be used for optimization. We will refer to this mode as <code>upper</code>. Let <span><span class="MathJax_Preview"> f(x) </span><script type="math/tex"> f(x) </script></span> be the ensembles prediction consisting of M models <span><span class="MathJax_Preview"> \{h^1, \dots, h^M\} </span><script type="math/tex"> \{h^1, \dots, h^M\} </script></span>, then </p>
<p><span><span class="MathJax_Preview">
\frac{1}{N}\sum_{j=1}^N \lambda \ell(f(x_j),y_j) + \frac{1-\lambda}{M}\sum_{i=1}^M \ell(h^i(x_j),y_j)
</span><script type="math/tex; mode=display">
\frac{1}{N}\sum_{j=1}^N \lambda \ell(f(x_j),y_j) + \frac{1-\lambda}{M}\sum_{i=1}^M \ell(h^i(x_j),y_j)
</script></span></p>
<p>where <span><span class="MathJax_Preview"> \lambda \in [0,1] </span><script type="math/tex"> \lambda \in [0,1] </script></span> is the trade-off between the ensemble's performance and its diversity.</p>
<p>If you are interested in explicitly computing the diversity we currently supports three loss functions: MSE, Negative Log-Likelihood and CrossEntropy. We call this mode <code>exact</code>. In this case the following objective is used:
<span><span class="MathJax_Preview">
\frac{1}{M} \sum_{i=1}^M \ell(h^i) - \frac{\lambda}{2M} \sum_{i=1}^M {d_i}^T D d_i
</span><script type="math/tex; mode=display">
\frac{1}{M} \sum_{i=1}^M \ell(h^i) - \frac{\lambda}{2M} \sum_{i=1}^M {d_i}^T D d_i
</script></span>
where <span><span class="MathJax_Preview"> D = \nabla^2_{f(x)}\ell(f(x), y), d_i = (h^i(x) - f(x))</span><script type="math/tex"> D = \nabla^2_{f(x)}\ell(f(x), y), d_i = (h^i(x) - f(x))</script></span> is optimized.</p>
<p>Note that PyTorchs autograd functionality does not directly support a hessian (or even gradients) on a "per example" basis, but only summed over a batch. So far I have not found a way to use autograd efficiently here. Therefore, this mode is currently restricted to the three mentioned loss functions.</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>n_estimators</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of estimators in ensemble. Should be at least 1</dd>
<dt><strong><code>l_reg</code></strong> :&ensp;<code>float</code></dt>
<dd>Trade-off between diversity and loss. Should be between 0 and 1. l_reg = 0 implies independent training whereas l_reg = 1 implies no independent training</dd>
<dt><strong><code>mode</code></strong> :&ensp;<code>str</code></dt>
<dd>
<p>The mode used for fitting a new model. </p>
<ul>
<li><code>mode != "exact"</code>: The upper bound is used for fitting. </li>
<li><code>mode == "exact"</code>: The diversity is explicity computed and used for fitting. Only <code>nn.MSELoss</code>, <code>nn.NLLLoss</code>, <code>nn.CrossEntropyLoss</code> are supported.</li>
</ul>
</dd>
</dl>
<p><strong>References</strong></p>
<p>[1] Liu, Y., &amp; Yao, X. (1999). Ensemble learning via negative correlation. Neural Networks, 12(10), 1399–1404. <a href="https://doi.org/10.1016/S0893-6080(99)00073-8">https://doi.org/10.1016/S0893-6080(99)00073-8</a> </p>
<p>[2] Brown, G., WatT, J. L., &amp; Tino, P. (2005). Managing Diversity in Regression Ensembles. Jmlr, (6), 1621–1650. <a href="https://doi.org/10.1097/IYC.0000000000000008">https://doi.org/10.1097/IYC.0000000000000008</a></p>
<p>[3] Opitz, M., Possegger, H., &amp; Bischof, H. (2016). Efficient model averaging for deep neural networks. Asian Conference on Computer Vision, 10112 LNCS, 205–220. <a href="https://doi.org/10.1007/978-3-319-54184-6_13">https://doi.org/10.1007/978-3-319-54184-6_13</a></p>
<p>[4] Shi, Z., Zhang, L., Liu, Y., Cao, X., Ye, Y., Cheng, M., &amp; Zheng, G. (n.d.). Crowd Counting with Deep Negative Correlation Learning, 5382–5390. Retrieved from <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Shi_Crowd_Counting_With_CVPR_2018_paper.pdf">http://openaccess.thecvf.com/content_cvpr_2018/papers/Shi_Crowd_Counting_With_CVPR_2018_paper.pdf</a></p>
<p>[5] Webb, A. M., Reynolds, C., Iliescu, D.-A., Reeve, H., Lujan, M., &amp; Brown, G. (2019). Joint Training of Neural Network Ensembles, (4), 1–14. <a href="https://doi.org/10.13140/RG.2.2.28091.46880">https://doi.org/10.13140/RG.2.2.28091.46880</a></p>
<p>[6] Webb, A. M., Reynolds, C., Chen, W., Reeve, H., Iliescu, D.-A., Lujan, M., &amp; Brown, G. (2020). To Ensemble or Not Ensemble: When does End-To-End Training Fail? In ECML PKDD 2020 (pp. 1–16). Retrieved from <a href="http://www.cs.man.ac.uk/~gbrown/publications/ecml2020webb.pdf">http://www.cs.man.ac.uk/~gbrown/publications/ecml2020webb.pdf</a></p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class GNCLClassifier(Ensemble):
    &#34;&#34;&#34;(Generalized) Negative Correlation Learning.

    Negative Correlation Learning uses the Bias-Variance-Co-Variance decomposition to derive a regularized objective function which enforces diversity among the ensemble members. The first appearance of this work was for the MSE loss in [1,2]. A modern application to the problem of crowd counting can be found in [4]. Optiz later proposed a similar objective using the Cross Entropy Loss, but without theoretical justifications [3]. Webb et al. in [5,6] gave more theoretical background for using the Cross Entropy Loss. Note that [6] is basically a shorter version of [5].

    We generalized and unified the previous works to include _any_ loss function using a second order Taylor approximation. As detailed in our paper, there is an upper bound for the GNCL objective which does not compute the exact diversity term. Thus, any differentiable loss \( \ell \) function can be used for optimization. We will refer to this mode as `upper`. Let \( f(x) \) be the ensembles prediction consisting of M models \( \{h^1, \dots, h^M\} \), then 

    $$
        \\frac{1}{N}\sum_{j=1}^N \lambda \ell(f(x_j),y_j) + \\frac{1-\lambda}{M}\sum_{i=1}^M \ell(h^i(x_j),y_j)
    $$

    where \( \lambda \in [0,1] \) is the trade-off between the ensemble&#39;s performance and its diversity.

    If you are interested in explicitly computing the diversity we currently supports three loss functions: MSE, Negative Log-Likelihood and CrossEntropy. We call this mode `exact`. In this case the following objective is used:
    $$
        \\frac{1}{M} \sum_{i=1}^M \ell(h^i) - \\frac{\lambda}{2M} \sum_{i=1}^M {d_i}^T D d_i
    $$
    where \( D = \\nabla^2_{f(x)}\ell(f(x), y), d_i = (h^i(x) - f(x))\) is optimized.

    Note that PyTorchs autograd functionality does not directly support a hessian (or even gradients) on a &#34;per example&#34; basis, but only summed over a batch. So far I have not found a way to use autograd efficiently here. Therefore, this mode is currently restricted to the three mentioned loss functions.
                
    Attributes:
        n_estimators (int): Number of estimators in ensemble. Should be at least 1
        
        l_reg (float): Trade-off between diversity and loss. Should be between 0 and 1. l_reg = 0 implies independent training whereas l_reg = 1 implies no independent training

        mode (str): The mode used for fitting a new model. 
            
            - `mode != &#34;exact&#34;`: The upper bound is used for fitting. 
            - `mode == &#34;exact&#34;`: The diversity is explicity computed and used for fitting. Only `nn.MSELoss`, `nn.NLLLoss`, `nn.CrossEntropyLoss` are supported.

    __References__

    [1] Liu, Y., &amp; Yao, X. (1999). Ensemble learning via negative correlation. Neural Networks, 12(10), 1399–1404. https://doi.org/10.1016/S0893-6080(99)00073-8 

    [2] Brown, G., WatT, J. L., &amp; Tino, P. (2005). Managing Diversity in Regression Ensembles. Jmlr, (6), 1621–1650. https://doi.org/10.1097/IYC.0000000000000008

    [3] Opitz, M., Possegger, H., &amp; Bischof, H. (2016). Efficient model averaging for deep neural networks. Asian Conference on Computer Vision, 10112 LNCS, 205–220. https://doi.org/10.1007/978-3-319-54184-6_13

    [4] Shi, Z., Zhang, L., Liu, Y., Cao, X., Ye, Y., Cheng, M., &amp; Zheng, G. (n.d.). Crowd Counting with Deep Negative Correlation Learning, 5382–5390. Retrieved from http://openaccess.thecvf.com/content_cvpr_2018/papers/Shi_Crowd_Counting_With_CVPR_2018_paper.pdf

    [5] Webb, A. M., Reynolds, C., Iliescu, D.-A., Reeve, H., Lujan, M., &amp; Brown, G. (2019). Joint Training of Neural Network Ensembles, (4), 1–14. https://doi.org/10.13140/RG.2.2.28091.46880

    [6] Webb, A. M., Reynolds, C., Chen, W., Reeve, H., Iliescu, D.-A., Lujan, M., &amp; Brown, G. (2020). To Ensemble or Not Ensemble: When does End-To-End Training Fail? In ECML PKDD 2020 (pp. 1–16). Retrieved from http://www.cs.man.ac.uk/~gbrown/publications/ecml2020webb.pdf
&#34;&#34;&#34;

    def __init__(self, l_reg = 0, mode = &#34;exact&#34;, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.l_reg = l_reg
        self.mode = mode
        self.estimators_ = nn.ModuleList([ self.base_estimator() for _ in range(self.n_estimators)])

        if self.mode == &#34;exact&#34; and not isinstance(self.loss_function, (nn.MSELoss, nn.NLLLoss, nn.CrossEntropyLoss)):
            warnings.warn(&#34;You set GNCL to &#39;exact&#39; but used an unsupported loss function for exact minimization. Currrently supported are MSELoss, NLLLoss, and CrossEntropyLoss. I am setting mode to &#39;upper&#39; now and minimize the upper bound using the provided loss function&#34;) 
            self.mode = &#34;upper&#34;

    def restore_state(self, checkpoint):
        super().restore_state(checkpoint)
        self.mode = checkpoint[&#34;mode&#34;]

    def get_state(self):
        state = super().get_state()
        return {
            **state,
            &#34;mode&#34;:self.mode,
        } 

    def prepare_backward(self, data, target, weights = None):
        # TODO Make use of the weights as well!
        f_bar, base_preds = self.forward_with_base(data)
        
        if self.mode == &#34;upper&#34;:
            n_classes = f_bar.shape[1]
            n_preds = f_bar.shape[0]
            D = torch.eye(n_classes).repeat(n_preds, 1, 1).cuda()
        else:
            if isinstance(self.loss_function, nn.MSELoss): 
                n_classes = f_bar.shape[1]
                n_preds = f_bar.shape[0]

                eye_matrix = torch.eye(n_classes).repeat(n_preds, 1, 1).cuda()
                D = 2.0*eye_matrix
            elif isinstance(self.loss_function, nn.NLLLoss):
                n_classes = f_bar.shape[1]
                n_preds = f_bar.shape[0]
                D = torch.eye(n_classes).repeat(n_preds, 1, 1).cuda()
                target_one_hot = torch.nn.functional.one_hot(target, num_classes = n_classes).type(self.get_float_type())

                eps = 1e-7
                diag_vector = target_one_hot*(1.0/(f_bar**2+eps))
                D.diagonal(dim1=-2, dim2=-1).copy_(diag_vector)
            elif isinstance(self.loss_function, nn.CrossEntropyLoss):
                n_preds = f_bar.shape[0]
                n_classes = f_bar.shape[1]
                f_bar_softmax = nn.functional.softmax(f_bar,dim=1)
                D = -1.0*torch.bmm(f_bar_softmax.unsqueeze(2), f_bar_softmax.unsqueeze(1))
                diag_vector = f_bar_softmax*(1.0-f_bar_softmax)
                D.diagonal(dim1=-2, dim2=-1).copy_(diag_vector)
            else:
                # NOTE: We should never reach this code path
                raise ValueError(&#34;Invalid combination of mode and loss function in GNCLClassifier.&#34;)

        losses = []
        accuracies = []
        diversity = []
        f_loss = self.loss_function(f_bar, target)
        for pred in base_preds:
            diff = pred - f_bar 
            covar = torch.bmm(diff.unsqueeze(1), torch.bmm(D, diff.unsqueeze(2))).squeeze()
            div = 1.0/self.n_estimators * 1.0/2.0 * covar
            i_loss = self.loss_function(pred, target)

            if self.mode == &#34;exact&#34;:
                # Eq. (4)
                reg_loss = 1.0/self.n_estimators * i_loss - self.l_reg * div
            else:
                # Eq. (5) where we scale the ensemble loss with 1.0/self.n_estimators due to the summation in line 118
                reg_loss = 1.0/self.n_estimators * self.l_reg * f_loss + (1.0 - self.l_reg)/self.n_estimators * i_loss
            
            losses.append(reg_loss)
            accuracies.append(100.0*(pred.argmax(1) == target).type(self.get_float_type()))
            diversity.append(div)

        losses = torch.stack(losses, dim = 1)
        accuracies = torch.stack(accuracies, dim = 1)
        diversity = torch.stack(diversity, dim = 1)
        
        # NOTE: avg loss is the average (regularized) loss and not the average loss (wrt. to the loss_function)
        d = {
            &#34;prediction&#34; : f_bar, 
            &#34;backward&#34; : losses.sum(dim=1), 
            &#34;metrics&#34; :
            {
                &#34;loss&#34; : f_loss,
                &#34;accuracy&#34; : 100.0*(f_bar.argmax(1) == target).type(self.get_float_type()), 
                &#34;avg loss&#34;: losses.mean(dim=1),
                &#34;avg accuracy&#34;: accuracies.mean(dim = 1),
                &#34;diversity&#34;: diversity.sum(dim = 1)
            } 
            
        }
        return d</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="pysembles.Models.Ensemble" href="Models.html#pysembles.Models.Ensemble">Ensemble</a></li>
<li><a title="pysembles.Models.BaseModel" href="Models.html#pysembles.Models.BaseModel">BaseModel</a></li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="pysembles.GNCLClassifier.GNCLClassifier.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="pysembles.GNCLClassifier.GNCLClassifier.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="pysembles.GNCLClassifier.GNCLClassifier.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, X) ‑> Callable[..., Any]</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, X):
    return self.forward_with_base(X)[0]</code></pre>
</details>
</dd>
<dt id="pysembles.GNCLClassifier.GNCLClassifier.get_state"><code class="name flex">
<span>def <span class="ident">get_state</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_state(self):
    state = super().get_state()
    return {
        **state,
        &#34;mode&#34;:self.mode,
    } </code></pre>
</details>
</dd>
<dt id="pysembles.GNCLClassifier.GNCLClassifier.prepare_backward"><code class="name flex">
<span>def <span class="ident">prepare_backward</span></span>(<span>self, data, target, weights=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def prepare_backward(self, data, target, weights = None):
    # TODO Make use of the weights as well!
    f_bar, base_preds = self.forward_with_base(data)
    
    if self.mode == &#34;upper&#34;:
        n_classes = f_bar.shape[1]
        n_preds = f_bar.shape[0]
        D = torch.eye(n_classes).repeat(n_preds, 1, 1).cuda()
    else:
        if isinstance(self.loss_function, nn.MSELoss): 
            n_classes = f_bar.shape[1]
            n_preds = f_bar.shape[0]

            eye_matrix = torch.eye(n_classes).repeat(n_preds, 1, 1).cuda()
            D = 2.0*eye_matrix
        elif isinstance(self.loss_function, nn.NLLLoss):
            n_classes = f_bar.shape[1]
            n_preds = f_bar.shape[0]
            D = torch.eye(n_classes).repeat(n_preds, 1, 1).cuda()
            target_one_hot = torch.nn.functional.one_hot(target, num_classes = n_classes).type(self.get_float_type())

            eps = 1e-7
            diag_vector = target_one_hot*(1.0/(f_bar**2+eps))
            D.diagonal(dim1=-2, dim2=-1).copy_(diag_vector)
        elif isinstance(self.loss_function, nn.CrossEntropyLoss):
            n_preds = f_bar.shape[0]
            n_classes = f_bar.shape[1]
            f_bar_softmax = nn.functional.softmax(f_bar,dim=1)
            D = -1.0*torch.bmm(f_bar_softmax.unsqueeze(2), f_bar_softmax.unsqueeze(1))
            diag_vector = f_bar_softmax*(1.0-f_bar_softmax)
            D.diagonal(dim1=-2, dim2=-1).copy_(diag_vector)
        else:
            # NOTE: We should never reach this code path
            raise ValueError(&#34;Invalid combination of mode and loss function in GNCLClassifier.&#34;)

    losses = []
    accuracies = []
    diversity = []
    f_loss = self.loss_function(f_bar, target)
    for pred in base_preds:
        diff = pred - f_bar 
        covar = torch.bmm(diff.unsqueeze(1), torch.bmm(D, diff.unsqueeze(2))).squeeze()
        div = 1.0/self.n_estimators * 1.0/2.0 * covar
        i_loss = self.loss_function(pred, target)

        if self.mode == &#34;exact&#34;:
            # Eq. (4)
            reg_loss = 1.0/self.n_estimators * i_loss - self.l_reg * div
        else:
            # Eq. (5) where we scale the ensemble loss with 1.0/self.n_estimators due to the summation in line 118
            reg_loss = 1.0/self.n_estimators * self.l_reg * f_loss + (1.0 - self.l_reg)/self.n_estimators * i_loss
        
        losses.append(reg_loss)
        accuracies.append(100.0*(pred.argmax(1) == target).type(self.get_float_type()))
        diversity.append(div)

    losses = torch.stack(losses, dim = 1)
    accuracies = torch.stack(accuracies, dim = 1)
    diversity = torch.stack(diversity, dim = 1)
    
    # NOTE: avg loss is the average (regularized) loss and not the average loss (wrt. to the loss_function)
    d = {
        &#34;prediction&#34; : f_bar, 
        &#34;backward&#34; : losses.sum(dim=1), 
        &#34;metrics&#34; :
        {
            &#34;loss&#34; : f_loss,
            &#34;accuracy&#34; : 100.0*(f_bar.argmax(1) == target).type(self.get_float_type()), 
            &#34;avg loss&#34;: losses.mean(dim=1),
            &#34;avg accuracy&#34;: accuracies.mean(dim = 1),
            &#34;diversity&#34;: diversity.sum(dim = 1)
        } 
        
    }
    return d</code></pre>
</details>
</dd>
<dt id="pysembles.GNCLClassifier.GNCLClassifier.restore_state"><code class="name flex">
<span>def <span class="ident">restore_state</span></span>(<span>self, checkpoint)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def restore_state(self, checkpoint):
    super().restore_state(checkpoint)
    self.mode = checkpoint[&#34;mode&#34;]</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="pysembles" href="index.html">pysembles</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="pysembles.GNCLClassifier.GNCLClassifier" href="#pysembles.GNCLClassifier.GNCLClassifier">GNCLClassifier</a></code></h4>
<ul class="two-column">
<li><code><a title="pysembles.GNCLClassifier.GNCLClassifier.dump_patches" href="#pysembles.GNCLClassifier.GNCLClassifier.dump_patches">dump_patches</a></code></li>
<li><code><a title="pysembles.GNCLClassifier.GNCLClassifier.forward" href="#pysembles.GNCLClassifier.GNCLClassifier.forward">forward</a></code></li>
<li><code><a title="pysembles.GNCLClassifier.GNCLClassifier.get_state" href="#pysembles.GNCLClassifier.GNCLClassifier.get_state">get_state</a></code></li>
<li><code><a title="pysembles.GNCLClassifier.GNCLClassifier.prepare_backward" href="#pysembles.GNCLClassifier.GNCLClassifier.prepare_backward">prepare_backward</a></code></li>
<li><code><a title="pysembles.GNCLClassifier.GNCLClassifier.restore_state" href="#pysembles.GNCLClassifier.GNCLClassifier.restore_state">restore_state</a></code></li>
<li><code><a title="pysembles.GNCLClassifier.GNCLClassifier.training" href="#pysembles.GNCLClassifier.GNCLClassifier.training">training</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>