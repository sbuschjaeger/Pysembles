<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>pysembles.BaggingClassifier API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS_CHTML" integrity="sha256-kZafAc6mZvK3W3v1pHOcUix30OHQN6pU/NO2oFkqZVw=" crossorigin></script>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>pysembles.BaggingClassifier</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">#!/usr/bin/env python3

import numpy as np
import torch

from torch import nn
from torch.utils.data import Sampler
from torch.utils.data import Dataset

from .Utils import TransformTensorDataset
from .Models import Ensemble, Model

import copy

class BootstrapSampler(Sampler):
    &#34;&#34;&#34; Implements bootstrap sampling in PyTorch
    Attributes:
        N (int): The total number of training datasets
        seed (long): The random seed
        bootstrap (bool): If true, sample with replacement else sample without replacement
        frac_examples (float): Fraction of training examples used for sampling N_sampled = (int) N * self.frac_examples. Must be from (0,1]. 
    &#34;&#34;&#34;
    def __init__(self, N, seed = 12345, bootstrap = True, frac_examples = 1.0):
        self.bootstrap = bootstrap
        self.frac_examples = frac_examples
        assert self.frac_samples &gt; 0 and self.frac_samples &lt;= 1.0, &#34;frac_examples expects the fraction of samples used, this must be between (0,1]. It was {}&#34;.format(self.frac_samples)

        np.random.seed(seed)
        idx_array = [i for i in range(N)]
        self.idx_sampled = np.random.choice(
            idx_array, 
            size=int(self.frac_examples*len(idx_array)), 
            replace=self.bootstrap
        )

    def __iter__(self):
        return iter(self.idx_sampled)

    def __len__(self):
        return len(self.idx_sampled)

class WeightedDataset(Dataset):
    &#34;&#34;&#34; A weighted dataset in PyTorch. Each example / target pair in the dataset receives a pre-computed weight. The dataset and the weight tensor should have the same length len(dataset) == len(w_tensor)
    
    Attributes:
        dataset: The original dataset
        w_tensor: A tensor of weights.
    &#34;&#34;&#34;
    def __init__(self, dataset, w_tensor):
        self.dataset = dataset
        self.w_tensor = w_tensor
        assert len(dataset) == len(w_tensor), &#34;Dataset and w_tensor should have the same size in WeightedDataset but received len(dataset) = {} and len(w_tensor) = {}&#34;.format(len(dataset, len(w_tensor)))

    def __getitem__(self, index):
        #items = self.dataset.__getitem__(index)
        items = self.dataset[index]
        weights = self.w_tensor[index]

        return (*items, weights)

    def __len__(self):
        return len(self.dataset)

class BaggingClassifier(Ensemble):
    &#34;&#34;&#34; Classic Bagging in the modern world of Deep Learning. 

    Bagging uses different subsets of features / training points to train an ensemble of classifiers [1]. The classic version of Bagging uses bootstrap samples which means that each base learner roughly receives 63% of the training data, whereas roughly 37% of the training data are duplicates. This lets each base model slightly overfit to their respective portion of the training data leading to a somewhat diverse ensemble. 

    This implementation supports a few variations of bagging. Similar to SKLearn you can choose the fraction of samples with and without bootstrapping. There is also a &#34;fast&#34; training method which jointly trains the ensemble using Poisson weights for each individual classifier. 

    Attributes:
        n_estimators (int): Number of estimators in the ensemble. Should be at least 1
        bootstrap (bool): If true, sampling is performed with replacement. If false, sampling is performed without replacement. Only has an effect if train_method = &#34;bagging&#34;
        frac_examples (float): Fraction of training examples used per base learner \( N\_base = (int) N * frac\_examples \) if N is the number of training data points. Must be from (0,1]. Only has an effect if train_method = &#34;bagging&#34;
        train_method (str): There are 3 modes:

            - `bagging`: The &#34;regular&#34; bagging-style training approach in which we compute bootstrap samples and train each estimators individually on its respective sample as presented in [1]. This trains one model after another which might be faster if the base models are already quite large and fully utilize the GPU. The size and type of sample can be controlled via `frac_examples` and `bootstrap` parameter. Please note, that currently this mode cannot be properly restored from a checkpoint.

            - `wagging`: Computes continuous Poisson weights which are used during fit as presented in [2]. This method can be faster for smaller base models which do not utilize the entire GPU. Moreover, we can follow the entire ensemble loss during optimization. It is important to note, that the Poisson weight is applied to the loss. The `frac_examples` and `bootstrap` parameters are ignored here.

            - `fast bagging` (or any other string which is not {`bagging`, `wagging`}). Computes discrete Poisson weights which are used during fit as presented in [3]. This method can be faster for smaller base models which do not utilize the entire GPU. Moreover, we can follow the entire ensemble loss during optimization. It is important to note, that the Poisson weight is applied to the loss. The `frac_examples` and `bootstrap` parameters are ignored here.

    __References__

    [1] Breiman, L. (1996). Bagging predictors. Machine Learning. https://doi.org/10.1007/bf00058655

    [2] Webb, G. I. (2000). MultiBoosting: a technique for combining boosting and wagging. Machine Learning. https://doi.org/10.1023/A:1007659514849

    [3] Oza, N. C., &amp; Russell, S. (2001). Online Bagging and Boosting. Retrieved from https://ti.arc.nasa.gov/m/profile/oza/files/ozru01a.pdf 
    &#34;&#34;&#34;
    def __init__(self, bootstrap = True, frac_examples = 1.0, train_method = &#34;fast bagging&#34;, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.frac_samples = frac_examples
        self.bootstrap = bootstrap
        self.train_method = train_method
        self.args = args
        self.kwargs = kwargs

        assert self.frac_samples &gt; 0 and self.frac_samples &lt;= 1.0, &#34;frac_examples expects the fraction of samples used, this must be between (0,1]. It was {}&#34;.format(self.frac_samples)

    def restore_state(self, checkpoint):
        super().restore_state(checkpoint)
        self.bootstrap = checkpoint[&#34;bootstrap&#34;]
        self.train_method = checkpoint[&#34;train_method&#34;]

    def get_state(self):
        state = super().get_state()
        return {
            **state,
            &#34;frac_samples&#34;:self.frac_samples,
            &#34;train_method&#34;:self.train_method
        } 

    def prepare_backward(self, data, target, weights = None):
        f_bar, base_preds = self.forward_with_base(data)

        accuracies = []
        losses = []
        for i, pred in enumerate(base_preds):
            # During training we set the weights to a Poisson distribution (see below).
            # However, during testing this function might also be executed. In this case, we 
            # dont want to weight models, but use all of them equally for computing statistics.
            if weights is None:
                iloss = self.loss_function(pred, target)
            else:
                # TODO: PyTorch copies the weight vector if we use weights[:,i] to index
                #       a specific row. Maybe we should re-factor this?
                #tmp = self.loss_function(pred, target)
                iloss = self.loss_function(pred, target) * weights[:,i].type(self.get_float_type())

            losses.append(iloss)
            accuracies.append(100.0*(pred.argmax(1) == target).type(self.get_float_type()))

        losses = torch.stack(losses, dim = 1)
        accuracies = torch.stack(accuracies, dim = 1)

        d = {
            &#34;prediction&#34; : f_bar, 
            &#34;backward&#34; : losses.sum(dim=1), 
            &#34;metrics&#34; :
            {
                &#34;loss&#34; : self.loss_function(f_bar, target),
                &#34;accuracy&#34; : 100.0*(f_bar.argmax(1) == target).type(self.get_float_type()), 
                &#34;avg loss&#34;: losses.mean(dim=1),
                &#34;avg accuracy&#34;: accuracies.mean(dim = 1)
            } 
            
        }
        return d

    def fit(self, data):
        if self.train_method == &#34;bagging&#34;:
            self.estimators_ = nn.ModuleList()

            # TODO Offer proper support for store / load from checkpoints
            for i in range(self.n_estimators):
                tmp_loader_cfg = self.loader_cfg
                tmp_loader_cfg[&#34;sampler&#34;] = BootstrapSampler(len(data), i, self.bootstrap, self.frac_examples)

                self.estimators_.append(
                    Model(loader= tmp_loader_cfg, training_file=&#34;training_{}.jsonl&#34;.format(i), *self.args, **self.kwargs)
                )
                self.estimators_[i].fit(data)

        else:
            self.estimators_ = nn.ModuleList([
                Model(training_file=&#34;training_{}.jsonl&#34;.format(i), *self.args, **self.kwargs) for i in range(self.n_estimators)
            ])

            if self.train_method == &#34;wagging&#34;:
                w_tensor = -torch.log( torch.randint(low=1,high=1000, size=(len(data), self.n_estimators)) / 1000.0 )
                w_dataset = WeightedDataset(data, w_tensor)
                super().fit(w_dataset)
            else:
                w_tensor = torch.poisson(torch.ones(size=(len(data), self.n_estimators)))
                w_dataset = WeightedDataset(data, w_tensor)
                super().fit(w_dataset)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="pysembles.BaggingClassifier.BaggingClassifier"><code class="flex name class">
<span>class <span class="ident">BaggingClassifier</span></span>
<span>(</span><span>bootstrap=True, frac_examples=1.0, train_method='fast bagging', *args, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Classic Bagging in the modern world of Deep Learning. </p>
<p>Bagging uses different subsets of features / training points to train an ensemble of classifiers [1]. The classic version of Bagging uses bootstrap samples which means that each base learner roughly receives 63% of the training data, whereas roughly 37% of the training data are duplicates. This lets each base model slightly overfit to their respective portion of the training data leading to a somewhat diverse ensemble. </p>
<p>This implementation supports a few variations of bagging. Similar to SKLearn you can choose the fraction of samples with and without bootstrapping. There is also a "fast" training method which jointly trains the ensemble using Poisson weights for each individual classifier. </p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>n_estimators</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of estimators in the ensemble. Should be at least 1</dd>
<dt><strong><code>bootstrap</code></strong> :&ensp;<code>bool</code></dt>
<dd>If true, sampling is performed with replacement. If false, sampling is performed without replacement. Only has an effect if train_method = "bagging"</dd>
<dt><strong><code>frac_examples</code></strong> :&ensp;<code>float</code></dt>
<dd>Fraction of training examples used per base learner <span><span class="MathJax_Preview"> N\_base = (int) N * frac\_examples </span><script type="math/tex"> N\_base = (int) N * frac\_examples </script></span> if N is the number of training data points. Must be from (0,1]. Only has an effect if train_method = "bagging"</dd>
<dt><strong><code>train_method</code></strong> :&ensp;<code>str</code></dt>
<dd>
<p>There are 3 modes:</p>
<ul>
<li>
<p><code>bagging</code>: The "regular" bagging-style training approach in which we compute bootstrap samples and train each estimators individually on its respective sample as presented in [1]. This trains one model after another which might be faster if the base models are already quite large and fully utilize the GPU. The size and type of sample can be controlled via <code>frac_examples</code> and <code>bootstrap</code> parameter. Please note, that currently this mode cannot be properly restored from a checkpoint.</p>
</li>
<li>
<p><code>wagging</code>: Computes continuous Poisson weights which are used during fit as presented in [2]. This method can be faster for smaller base models which do not utilize the entire GPU. Moreover, we can follow the entire ensemble loss during optimization. It is important to note, that the Poisson weight is applied to the loss. The <code>frac_examples</code> and <code>bootstrap</code> parameters are ignored here.</p>
</li>
<li>
<p><code>fast bagging</code> (or any other string which is not {<code>bagging</code>, <code>wagging</code>}). Computes discrete Poisson weights which are used during fit as presented in [3]. This method can be faster for smaller base models which do not utilize the entire GPU. Moreover, we can follow the entire ensemble loss during optimization. It is important to note, that the Poisson weight is applied to the loss. The <code>frac_examples</code> and <code>bootstrap</code> parameters are ignored here.</p>
</li>
</ul>
</dd>
</dl>
<p><strong>References</strong></p>
<p>[1] Breiman, L. (1996). Bagging predictors. Machine Learning. <a href="https://doi.org/10.1007/bf00058655">https://doi.org/10.1007/bf00058655</a></p>
<p>[2] Webb, G. I. (2000). MultiBoosting: a technique for combining boosting and wagging. Machine Learning. <a href="https://doi.org/10.1023/A:1007659514849">https://doi.org/10.1023/A:1007659514849</a></p>
<p>[3] Oza, N. C., &amp; Russell, S. (2001). Online Bagging and Boosting. Retrieved from <a href="https://ti.arc.nasa.gov/m/profile/oza/files/ozru01a.pdf">https://ti.arc.nasa.gov/m/profile/oza/files/ozru01a.pdf</a> </p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class BaggingClassifier(Ensemble):
    &#34;&#34;&#34; Classic Bagging in the modern world of Deep Learning. 

    Bagging uses different subsets of features / training points to train an ensemble of classifiers [1]. The classic version of Bagging uses bootstrap samples which means that each base learner roughly receives 63% of the training data, whereas roughly 37% of the training data are duplicates. This lets each base model slightly overfit to their respective portion of the training data leading to a somewhat diverse ensemble. 

    This implementation supports a few variations of bagging. Similar to SKLearn you can choose the fraction of samples with and without bootstrapping. There is also a &#34;fast&#34; training method which jointly trains the ensemble using Poisson weights for each individual classifier. 

    Attributes:
        n_estimators (int): Number of estimators in the ensemble. Should be at least 1
        bootstrap (bool): If true, sampling is performed with replacement. If false, sampling is performed without replacement. Only has an effect if train_method = &#34;bagging&#34;
        frac_examples (float): Fraction of training examples used per base learner \( N\_base = (int) N * frac\_examples \) if N is the number of training data points. Must be from (0,1]. Only has an effect if train_method = &#34;bagging&#34;
        train_method (str): There are 3 modes:

            - `bagging`: The &#34;regular&#34; bagging-style training approach in which we compute bootstrap samples and train each estimators individually on its respective sample as presented in [1]. This trains one model after another which might be faster if the base models are already quite large and fully utilize the GPU. The size and type of sample can be controlled via `frac_examples` and `bootstrap` parameter. Please note, that currently this mode cannot be properly restored from a checkpoint.

            - `wagging`: Computes continuous Poisson weights which are used during fit as presented in [2]. This method can be faster for smaller base models which do not utilize the entire GPU. Moreover, we can follow the entire ensemble loss during optimization. It is important to note, that the Poisson weight is applied to the loss. The `frac_examples` and `bootstrap` parameters are ignored here.

            - `fast bagging` (or any other string which is not {`bagging`, `wagging`}). Computes discrete Poisson weights which are used during fit as presented in [3]. This method can be faster for smaller base models which do not utilize the entire GPU. Moreover, we can follow the entire ensemble loss during optimization. It is important to note, that the Poisson weight is applied to the loss. The `frac_examples` and `bootstrap` parameters are ignored here.

    __References__

    [1] Breiman, L. (1996). Bagging predictors. Machine Learning. https://doi.org/10.1007/bf00058655

    [2] Webb, G. I. (2000). MultiBoosting: a technique for combining boosting and wagging. Machine Learning. https://doi.org/10.1023/A:1007659514849

    [3] Oza, N. C., &amp; Russell, S. (2001). Online Bagging and Boosting. Retrieved from https://ti.arc.nasa.gov/m/profile/oza/files/ozru01a.pdf 
    &#34;&#34;&#34;
    def __init__(self, bootstrap = True, frac_examples = 1.0, train_method = &#34;fast bagging&#34;, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.frac_samples = frac_examples
        self.bootstrap = bootstrap
        self.train_method = train_method
        self.args = args
        self.kwargs = kwargs

        assert self.frac_samples &gt; 0 and self.frac_samples &lt;= 1.0, &#34;frac_examples expects the fraction of samples used, this must be between (0,1]. It was {}&#34;.format(self.frac_samples)

    def restore_state(self, checkpoint):
        super().restore_state(checkpoint)
        self.bootstrap = checkpoint[&#34;bootstrap&#34;]
        self.train_method = checkpoint[&#34;train_method&#34;]

    def get_state(self):
        state = super().get_state()
        return {
            **state,
            &#34;frac_samples&#34;:self.frac_samples,
            &#34;train_method&#34;:self.train_method
        } 

    def prepare_backward(self, data, target, weights = None):
        f_bar, base_preds = self.forward_with_base(data)

        accuracies = []
        losses = []
        for i, pred in enumerate(base_preds):
            # During training we set the weights to a Poisson distribution (see below).
            # However, during testing this function might also be executed. In this case, we 
            # dont want to weight models, but use all of them equally for computing statistics.
            if weights is None:
                iloss = self.loss_function(pred, target)
            else:
                # TODO: PyTorch copies the weight vector if we use weights[:,i] to index
                #       a specific row. Maybe we should re-factor this?
                #tmp = self.loss_function(pred, target)
                iloss = self.loss_function(pred, target) * weights[:,i].type(self.get_float_type())

            losses.append(iloss)
            accuracies.append(100.0*(pred.argmax(1) == target).type(self.get_float_type()))

        losses = torch.stack(losses, dim = 1)
        accuracies = torch.stack(accuracies, dim = 1)

        d = {
            &#34;prediction&#34; : f_bar, 
            &#34;backward&#34; : losses.sum(dim=1), 
            &#34;metrics&#34; :
            {
                &#34;loss&#34; : self.loss_function(f_bar, target),
                &#34;accuracy&#34; : 100.0*(f_bar.argmax(1) == target).type(self.get_float_type()), 
                &#34;avg loss&#34;: losses.mean(dim=1),
                &#34;avg accuracy&#34;: accuracies.mean(dim = 1)
            } 
            
        }
        return d

    def fit(self, data):
        if self.train_method == &#34;bagging&#34;:
            self.estimators_ = nn.ModuleList()

            # TODO Offer proper support for store / load from checkpoints
            for i in range(self.n_estimators):
                tmp_loader_cfg = self.loader_cfg
                tmp_loader_cfg[&#34;sampler&#34;] = BootstrapSampler(len(data), i, self.bootstrap, self.frac_examples)

                self.estimators_.append(
                    Model(loader= tmp_loader_cfg, training_file=&#34;training_{}.jsonl&#34;.format(i), *self.args, **self.kwargs)
                )
                self.estimators_[i].fit(data)

        else:
            self.estimators_ = nn.ModuleList([
                Model(training_file=&#34;training_{}.jsonl&#34;.format(i), *self.args, **self.kwargs) for i in range(self.n_estimators)
            ])

            if self.train_method == &#34;wagging&#34;:
                w_tensor = -torch.log( torch.randint(low=1,high=1000, size=(len(data), self.n_estimators)) / 1000.0 )
                w_dataset = WeightedDataset(data, w_tensor)
                super().fit(w_dataset)
            else:
                w_tensor = torch.poisson(torch.ones(size=(len(data), self.n_estimators)))
                w_dataset = WeightedDataset(data, w_tensor)
                super().fit(w_dataset)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="pysembles.Models.Ensemble" href="Models.html#pysembles.Models.Ensemble">Ensemble</a></li>
<li><a title="pysembles.Models.BaseModel" href="Models.html#pysembles.Models.BaseModel">BaseModel</a></li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="pysembles.BaggingClassifier.BaggingClassifier.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="pysembles.BaggingClassifier.BaggingClassifier.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="pysembles.BaggingClassifier.BaggingClassifier.fit"><code class="name flex">
<span>def <span class="ident">fit</span></span>(<span>self, data)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fit(self, data):
    if self.train_method == &#34;bagging&#34;:
        self.estimators_ = nn.ModuleList()

        # TODO Offer proper support for store / load from checkpoints
        for i in range(self.n_estimators):
            tmp_loader_cfg = self.loader_cfg
            tmp_loader_cfg[&#34;sampler&#34;] = BootstrapSampler(len(data), i, self.bootstrap, self.frac_examples)

            self.estimators_.append(
                Model(loader= tmp_loader_cfg, training_file=&#34;training_{}.jsonl&#34;.format(i), *self.args, **self.kwargs)
            )
            self.estimators_[i].fit(data)

    else:
        self.estimators_ = nn.ModuleList([
            Model(training_file=&#34;training_{}.jsonl&#34;.format(i), *self.args, **self.kwargs) for i in range(self.n_estimators)
        ])

        if self.train_method == &#34;wagging&#34;:
            w_tensor = -torch.log( torch.randint(low=1,high=1000, size=(len(data), self.n_estimators)) / 1000.0 )
            w_dataset = WeightedDataset(data, w_tensor)
            super().fit(w_dataset)
        else:
            w_tensor = torch.poisson(torch.ones(size=(len(data), self.n_estimators)))
            w_dataset = WeightedDataset(data, w_tensor)
            super().fit(w_dataset)</code></pre>
</details>
</dd>
<dt id="pysembles.BaggingClassifier.BaggingClassifier.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, X) ‑> Callable[..., Any]</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, X):
    return self.forward_with_base(X)[0]</code></pre>
</details>
</dd>
<dt id="pysembles.BaggingClassifier.BaggingClassifier.get_state"><code class="name flex">
<span>def <span class="ident">get_state</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_state(self):
    state = super().get_state()
    return {
        **state,
        &#34;frac_samples&#34;:self.frac_samples,
        &#34;train_method&#34;:self.train_method
    } </code></pre>
</details>
</dd>
<dt id="pysembles.BaggingClassifier.BaggingClassifier.prepare_backward"><code class="name flex">
<span>def <span class="ident">prepare_backward</span></span>(<span>self, data, target, weights=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def prepare_backward(self, data, target, weights = None):
    f_bar, base_preds = self.forward_with_base(data)

    accuracies = []
    losses = []
    for i, pred in enumerate(base_preds):
        # During training we set the weights to a Poisson distribution (see below).
        # However, during testing this function might also be executed. In this case, we 
        # dont want to weight models, but use all of them equally for computing statistics.
        if weights is None:
            iloss = self.loss_function(pred, target)
        else:
            # TODO: PyTorch copies the weight vector if we use weights[:,i] to index
            #       a specific row. Maybe we should re-factor this?
            #tmp = self.loss_function(pred, target)
            iloss = self.loss_function(pred, target) * weights[:,i].type(self.get_float_type())

        losses.append(iloss)
        accuracies.append(100.0*(pred.argmax(1) == target).type(self.get_float_type()))

    losses = torch.stack(losses, dim = 1)
    accuracies = torch.stack(accuracies, dim = 1)

    d = {
        &#34;prediction&#34; : f_bar, 
        &#34;backward&#34; : losses.sum(dim=1), 
        &#34;metrics&#34; :
        {
            &#34;loss&#34; : self.loss_function(f_bar, target),
            &#34;accuracy&#34; : 100.0*(f_bar.argmax(1) == target).type(self.get_float_type()), 
            &#34;avg loss&#34;: losses.mean(dim=1),
            &#34;avg accuracy&#34;: accuracies.mean(dim = 1)
        } 
        
    }
    return d</code></pre>
</details>
</dd>
<dt id="pysembles.BaggingClassifier.BaggingClassifier.restore_state"><code class="name flex">
<span>def <span class="ident">restore_state</span></span>(<span>self, checkpoint)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def restore_state(self, checkpoint):
    super().restore_state(checkpoint)
    self.bootstrap = checkpoint[&#34;bootstrap&#34;]
    self.train_method = checkpoint[&#34;train_method&#34;]</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="pysembles.BaggingClassifier.BootstrapSampler"><code class="flex name class">
<span>class <span class="ident">BootstrapSampler</span></span>
<span>(</span><span>N, seed=12345, bootstrap=True, frac_examples=1.0)</span>
</code></dt>
<dd>
<div class="desc"><p>Implements bootstrap sampling in PyTorch</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>N</code></strong> :&ensp;<code>int</code></dt>
<dd>The total number of training datasets</dd>
<dt><strong><code>seed</code></strong> :&ensp;<code>long</code></dt>
<dd>The random seed</dd>
<dt><strong><code>bootstrap</code></strong> :&ensp;<code>bool</code></dt>
<dd>If true, sample with replacement else sample without replacement</dd>
<dt><strong><code>frac_examples</code></strong> :&ensp;<code>float</code></dt>
<dd>Fraction of training examples used for sampling N_sampled = (int) N * self.frac_examples. Must be from (0,1].</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class BootstrapSampler(Sampler):
    &#34;&#34;&#34; Implements bootstrap sampling in PyTorch
    Attributes:
        N (int): The total number of training datasets
        seed (long): The random seed
        bootstrap (bool): If true, sample with replacement else sample without replacement
        frac_examples (float): Fraction of training examples used for sampling N_sampled = (int) N * self.frac_examples. Must be from (0,1]. 
    &#34;&#34;&#34;
    def __init__(self, N, seed = 12345, bootstrap = True, frac_examples = 1.0):
        self.bootstrap = bootstrap
        self.frac_examples = frac_examples
        assert self.frac_samples &gt; 0 and self.frac_samples &lt;= 1.0, &#34;frac_examples expects the fraction of samples used, this must be between (0,1]. It was {}&#34;.format(self.frac_samples)

        np.random.seed(seed)
        idx_array = [i for i in range(N)]
        self.idx_sampled = np.random.choice(
            idx_array, 
            size=int(self.frac_examples*len(idx_array)), 
            replace=self.bootstrap
        )

    def __iter__(self):
        return iter(self.idx_sampled)

    def __len__(self):
        return len(self.idx_sampled)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.utils.data.sampler.Sampler</li>
</ul>
</dd>
<dt id="pysembles.BaggingClassifier.WeightedDataset"><code class="flex name class">
<span>class <span class="ident">WeightedDataset</span></span>
<span>(</span><span>dataset, w_tensor)</span>
</code></dt>
<dd>
<div class="desc"><p>A weighted dataset in PyTorch. Each example / target pair in the dataset receives a pre-computed weight. The dataset and the weight tensor should have the same length len(dataset) == len(w_tensor)</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>dataset</code></strong></dt>
<dd>The original dataset</dd>
<dt><strong><code>w_tensor</code></strong></dt>
<dd>A tensor of weights.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class WeightedDataset(Dataset):
    &#34;&#34;&#34; A weighted dataset in PyTorch. Each example / target pair in the dataset receives a pre-computed weight. The dataset and the weight tensor should have the same length len(dataset) == len(w_tensor)
    
    Attributes:
        dataset: The original dataset
        w_tensor: A tensor of weights.
    &#34;&#34;&#34;
    def __init__(self, dataset, w_tensor):
        self.dataset = dataset
        self.w_tensor = w_tensor
        assert len(dataset) == len(w_tensor), &#34;Dataset and w_tensor should have the same size in WeightedDataset but received len(dataset) = {} and len(w_tensor) = {}&#34;.format(len(dataset, len(w_tensor)))

    def __getitem__(self, index):
        #items = self.dataset.__getitem__(index)
        items = self.dataset[index]
        weights = self.w_tensor[index]

        return (*items, weights)

    def __len__(self):
        return len(self.dataset)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.utils.data.dataset.Dataset</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="pysembles" href="index.html">pysembles</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="pysembles.BaggingClassifier.BaggingClassifier" href="#pysembles.BaggingClassifier.BaggingClassifier">BaggingClassifier</a></code></h4>
<ul class="two-column">
<li><code><a title="pysembles.BaggingClassifier.BaggingClassifier.dump_patches" href="#pysembles.BaggingClassifier.BaggingClassifier.dump_patches">dump_patches</a></code></li>
<li><code><a title="pysembles.BaggingClassifier.BaggingClassifier.fit" href="#pysembles.BaggingClassifier.BaggingClassifier.fit">fit</a></code></li>
<li><code><a title="pysembles.BaggingClassifier.BaggingClassifier.forward" href="#pysembles.BaggingClassifier.BaggingClassifier.forward">forward</a></code></li>
<li><code><a title="pysembles.BaggingClassifier.BaggingClassifier.get_state" href="#pysembles.BaggingClassifier.BaggingClassifier.get_state">get_state</a></code></li>
<li><code><a title="pysembles.BaggingClassifier.BaggingClassifier.prepare_backward" href="#pysembles.BaggingClassifier.BaggingClassifier.prepare_backward">prepare_backward</a></code></li>
<li><code><a title="pysembles.BaggingClassifier.BaggingClassifier.restore_state" href="#pysembles.BaggingClassifier.BaggingClassifier.restore_state">restore_state</a></code></li>
<li><code><a title="pysembles.BaggingClassifier.BaggingClassifier.training" href="#pysembles.BaggingClassifier.BaggingClassifier.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="pysembles.BaggingClassifier.BootstrapSampler" href="#pysembles.BaggingClassifier.BootstrapSampler">BootstrapSampler</a></code></h4>
</li>
<li>
<h4><code><a title="pysembles.BaggingClassifier.WeightedDataset" href="#pysembles.BaggingClassifier.WeightedDataset">WeightedDataset</a></code></h4>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>